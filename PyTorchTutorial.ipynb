{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial Workthrough\n",
    "\n",
    "Here we work through a basic example in PyTorch. It is the example used in the [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html). The material heavily relies on the excellent material in that tutorial. The presentation is similar but with a slightly different emphasis. In particular this is written from the perspective of an econometrician and some parallels and differences to traditional econometric techniques will be pointed out.\n",
    "\n",
    "\n",
    "## Load some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from datetime import datetime\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "The PyTorch library expects data inputs as tensors. For example, here we load some inflation data. This example will actually not work with inflation data, but this is just to get your thinking about the data structure going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inflation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960-01-01</th>\n",
       "      <td>-0.340136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-01-02</th>\n",
       "      <td>0.341297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-01-03</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-01-04</th>\n",
       "      <td>0.340136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>0.331073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>0.505904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>0.251844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>0.322891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-07</th>\n",
       "      <td>0.190752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            inflation\n",
       "DATE                 \n",
       "1960-01-01  -0.340136\n",
       "1960-01-02   0.341297\n",
       "1960-01-03   0.000000\n",
       "1960-01-04   0.340136\n",
       "1960-01-05   0.000000\n",
       "...               ...\n",
       "2023-01-03   0.331073\n",
       "2023-01-04   0.505904\n",
       "2023-01-05   0.251844\n",
       "2023-01-06   0.322891\n",
       "2023-01-07   0.190752\n",
       "\n",
       "[763 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('datasets/CPALTT01USM657N.csv')\n",
    "data.tail()\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data.set_index('DATE', inplace=True)\n",
    "data.rename(columns={'CPALTT01USM657N': 'inflation'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be transformed into a tensor like so, where we use the `values()` property of the dataframe. The resulting tensor has properties `shape` and `type` which give you the relevant shape and type information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([763, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor(data.values)\n",
    "tensor.dtype\n",
    "tensor.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are basically multi-dimensional matrices and you can do arithmatic with these. Find a [list of operations](https://pytorch.org/docs/stable/torch.html) to see what is available. In this example we will not work with these inflation data any longer, but we wanted to show that data you are very familiar with can easily be turned into tensor data. This is important as the `torch` package we will use for estimating neural network models, does require the use of tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "\n",
    "To follow the example in the PyTorch tutorial we need to load the Fashion-MNIST dataset. This dataset is already in the right format and we obtain a training and a testing portion for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This split the dataset into a set of 60,000 images to train the network and a set of 10,000 images to evaluate the quality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are image data, for instance the first image `training_data[1]` gives you information on the 28x28 pixels of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1608, 0.7373, 0.4039, 0.2118, 0.1882, 0.1686,\n",
       "           0.3412, 0.6588, 0.5216, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.5333, 0.8588, 0.8471, 0.8941, 0.9255, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 0.8510, 0.8431, 0.9961, 0.9059, 0.6275, 0.1765, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.6902, 0.8706,\n",
       "           0.8784, 0.8314, 0.7961, 0.7765, 0.7686, 0.7843, 0.8431, 0.8000,\n",
       "           0.7922, 0.7882, 0.7882, 0.7882, 0.8196, 0.8549, 0.8784, 0.6431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 0.8588, 0.7843,\n",
       "           0.7765, 0.7922, 0.7765, 0.7804, 0.7804, 0.7882, 0.7686, 0.7765,\n",
       "           0.7765, 0.7843, 0.7843, 0.7843, 0.7843, 0.7882, 0.7843, 0.8824,\n",
       "           0.1608, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.8588, 0.7804, 0.7961,\n",
       "           0.7961, 0.8314, 0.9333, 0.9725, 0.9804, 0.9608, 0.9765, 0.9647,\n",
       "           0.9686, 0.9882, 0.9725, 0.9216, 0.8118, 0.7961, 0.7961, 0.8706,\n",
       "           0.5490, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.4549, 0.8863, 0.8078, 0.8000,\n",
       "           0.8118, 0.8000, 0.3961, 0.2941, 0.1843, 0.2863, 0.1882, 0.1961,\n",
       "           0.1765, 0.2000, 0.2471, 0.4431, 0.8706, 0.7922, 0.8078, 0.8627,\n",
       "           0.8784, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.7843, 0.8706, 0.8196, 0.7961,\n",
       "           0.8431, 0.7843, 0.0000, 0.2745, 0.3843, 0.0000, 0.4039, 0.2314,\n",
       "           0.2667, 0.2784, 0.1922, 0.0000, 0.8588, 0.8078, 0.8392, 0.8235,\n",
       "           0.9804, 0.1490, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.9686, 0.8549, 0.8314, 0.8235,\n",
       "           0.8431, 0.8392, 0.0000, 0.9961, 0.9529, 0.5451, 1.0000, 0.6824,\n",
       "           0.9843, 1.0000, 0.8039, 0.0000, 0.8431, 0.8510, 0.8392, 0.8157,\n",
       "           0.8627, 0.3725, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.1765, 0.8863, 0.8392, 0.8392, 0.8431,\n",
       "           0.8784, 0.8039, 0.0000, 0.1647, 0.1373, 0.2353, 0.0627, 0.0667,\n",
       "           0.0471, 0.0510, 0.2745, 0.0000, 0.7412, 0.8471, 0.8314, 0.8078,\n",
       "           0.8314, 0.6118, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.6431, 0.9216, 0.8392, 0.8275, 0.8627,\n",
       "           0.8471, 0.7882, 0.2039, 0.2784, 0.3490, 0.3686, 0.3255, 0.3059,\n",
       "           0.2745, 0.2980, 0.3608, 0.3412, 0.8078, 0.8118, 0.8706, 0.8353,\n",
       "           0.8588, 0.8157, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.4157, 0.7333, 0.8745, 0.9294, 0.9725,\n",
       "           0.8275, 0.7765, 0.9882, 0.9804, 0.9725, 0.9608, 0.9725, 0.9882,\n",
       "           0.9922, 0.9804, 0.9882, 0.9373, 0.7882, 0.8314, 0.8824, 0.8431,\n",
       "           0.7569, 0.4431, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.2118, 0.6235,\n",
       "           0.8706, 0.7569, 0.8157, 0.7529, 0.7725, 0.7843, 0.7843, 0.7843,\n",
       "           0.7843, 0.7882, 0.7961, 0.7647, 0.8235, 0.6471, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1843,\n",
       "           0.8824, 0.7529, 0.8392, 0.7961, 0.8078, 0.8000, 0.8000, 0.8039,\n",
       "           0.8078, 0.8000, 0.8314, 0.7725, 0.8549, 0.4196, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0235, 0.0000, 0.1804,\n",
       "           0.8314, 0.7647, 0.8314, 0.7922, 0.8078, 0.8039, 0.8000, 0.8039,\n",
       "           0.8078, 0.8000, 0.8314, 0.7843, 0.8549, 0.3569, 0.0000, 0.0118,\n",
       "           0.0039, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0431,\n",
       "           0.7725, 0.7804, 0.8039, 0.7922, 0.8039, 0.8078, 0.8000, 0.8039,\n",
       "           0.8118, 0.8000, 0.8039, 0.8039, 0.8549, 0.3020, 0.0000, 0.0196,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.0078,\n",
       "           0.7490, 0.7765, 0.7882, 0.8039, 0.8078, 0.8039, 0.8039, 0.8078,\n",
       "           0.8196, 0.8078, 0.7804, 0.8196, 0.8588, 0.2902, 0.0000, 0.0196,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000,\n",
       "           0.7373, 0.7725, 0.7843, 0.8118, 0.8118, 0.8000, 0.8118, 0.8118,\n",
       "           0.8235, 0.8157, 0.7765, 0.8118, 0.8667, 0.2824, 0.0000, 0.0157,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000,\n",
       "           0.8431, 0.7765, 0.7961, 0.8078, 0.8157, 0.8039, 0.8118, 0.8118,\n",
       "           0.8235, 0.8157, 0.7843, 0.7922, 0.8706, 0.2941, 0.0000, 0.0157,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.8314, 0.7765, 0.8196, 0.8078, 0.8196, 0.8078, 0.8157, 0.8118,\n",
       "           0.8275, 0.8078, 0.8039, 0.7765, 0.8667, 0.3137, 0.0000, 0.0118,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.8000, 0.7882, 0.8039, 0.8157, 0.8118, 0.8039, 0.8275, 0.8039,\n",
       "           0.8235, 0.8235, 0.8196, 0.7647, 0.8667, 0.3765, 0.0000, 0.0118,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.7922, 0.7882, 0.8039, 0.8196, 0.8118, 0.8039, 0.8353, 0.8078,\n",
       "           0.8235, 0.8196, 0.8235, 0.7608, 0.8510, 0.4118, 0.0000, 0.0078,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.8000, 0.8000, 0.8039, 0.8157, 0.8118, 0.8039, 0.8431, 0.8118,\n",
       "           0.8235, 0.8157, 0.8275, 0.7569, 0.8353, 0.4510, 0.0000, 0.0078,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.8000, 0.8118, 0.8118, 0.8157, 0.8078, 0.8078, 0.8431, 0.8235,\n",
       "           0.8235, 0.8118, 0.8314, 0.7647, 0.8235, 0.4627, 0.0000, 0.0078,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.7765, 0.8157, 0.8157, 0.8157, 0.8000, 0.8118, 0.8314, 0.8314,\n",
       "           0.8235, 0.8118, 0.8275, 0.7686, 0.8118, 0.4745, 0.0000, 0.0039,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.7765, 0.8235, 0.8118, 0.8157, 0.8078, 0.8196, 0.8353, 0.8314,\n",
       "           0.8275, 0.8118, 0.8235, 0.7725, 0.8118, 0.4863, 0.0000, 0.0039,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6745, 0.8235, 0.7961, 0.7882, 0.7804, 0.8000, 0.8118, 0.8039,\n",
       "           0.8000, 0.7882, 0.8039, 0.7725, 0.8078, 0.4980, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.7373, 0.8667, 0.8392, 0.9176, 0.9255, 0.9333, 0.9569, 0.9569,\n",
       "           0.9569, 0.9412, 0.9529, 0.8392, 0.8784, 0.6353, 0.0000, 0.0078,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "           0.5451, 0.5725, 0.5098, 0.5294, 0.5294, 0.5373, 0.4902, 0.4863,\n",
       "           0.4902, 0.4745, 0.4667, 0.4471, 0.5098, 0.2980, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data avaialable in `test_data` and `training_data`. For certain operations it will turn out to be useful to access the data via the DataLoader. This will enable access samples of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indicated above that, if iterating through the data, we get random items. Each item has an image and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = training_data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations\n",
    "\n",
    "Neural Network models need data input in very controlled scales. Examples are standardised variables (mean 0 and variance 1) or data mapped into a $[0,1]$ interval. In fact, when we imported the Fashion data earlier, we called `transform=ToTensor()`. This ensured that the pixel information was mapped into the $[0,1]$ intervall. The `target` variable, which is the label, however, came as an integer in $[0,9]$. By reloading the data with the `target_transform` as below, we change the label into 10 indicator variables with all but one taking the value 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "ds_test = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the 15th item out of `ds_train`, show it and see what type of item it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "img, label = ds_train[14]\n",
    "img = img.squeeze()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the third last indicator variable is 1. That corresponds to label 7, a sneaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Now that the data are prepared we need to build the model. This is done using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will return to the details of the above setup a little later. For now just accept it.\n",
    "\n",
    "But what is useful to understand what the above code did. It defined a `class` called `NeuralNetwork`. This will allow us to soon define a particular object (in computing speak an \"instance\" of that type of object) of that class. More on this later. \n",
    "\n",
    "Estimating, or training in the language of machine learning, a neural network model can by computing intensive. So we want to use the best possible hardware to do so. The following bit of code checks whether you have any special resources available. If not, then your computer's CPU will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall define one instance of the `NeuralNetwork` type of objects, and we call it `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model eventually maps the 784 (28x28 pixels) into 10 output items (the 10 different labels). In other words, we feed in an image (as defined by 728 pixels) and we receive information in terms of 10 outputs (the 10 possible labels).\n",
    "\n",
    "Let us now define a random image and then we print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "plt.imshow(X.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is just noise. We shouldn't expect to see any particular item of clothing in here. But anyways, let's see whether our Neural Network can detect anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0955, 0.0983, 0.0974, 0.0995, 0.0953, 0.0941, 0.1086, 0.1120, 0.1022,\n",
      "         0.0971]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see here are the predicted probabilities for each of the 10 item classes (from t-shirt to ankle boots). You can see that these probabilities are all between 8 and 11% and in fact they sum to 1. This is not surprising as the random image does not look anythink like any of the items in our list. The max prob here is the probability of 10.55% for a bag.\n",
    "\n",
    "Let's see how the model does with one of our images from the test dataset (`ds_test`). We will pick the 15th image from the training dataset. We'll first print the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "img, label = ds_train[15]\n",
    "img = img.squeeze()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an image of an ankle boot. Now we feed this into our network. Will it identify the picture as an ankle boot? `ds_test[15]` picks the 15th item but there are two elements, the image as the first element (`[0]`) and the label as the second element (`[1]`). So `ds_test[15][0]` picks the image only as we feed it into `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0961, 0.1032, 0.0987, 0.0951, 0.1032, 0.0966, 0.1068, 0.1069, 0.0997,\n",
      "         0.0936]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits = model(ds_test[15][0])\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is basically as for the random image. All labels have a probability around 10%. Why is that. Well, so far we only initiated a network architecture (which includes some random model coefficients). The network has not seen any data yet which it could use to optimise the parameters such that it recognises an ankle boot.\n",
    "\n",
    "In fact you can see what the parameters for the model are (as we initialised it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0125, -0.0350, -0.0087,  ...,  0.0161,  0.0084, -0.0049],\n",
      "        [ 0.0058, -0.0292,  0.0140,  ...,  0.0200, -0.0187, -0.0088]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0337, -0.0068], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0048,  0.0296, -0.0355,  ...,  0.0434, -0.0135, -0.0408],\n",
      "        [ 0.0405, -0.0185,  0.0233,  ..., -0.0094, -0.0402, -0.0103]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0322, -0.0193], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0253, -0.0168, -0.0208,  ..., -0.0061,  0.0268, -0.0401],\n",
      "        [-0.0283, -0.0097,  0.0375,  ..., -0.0025,  0.0085,  0.0235]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0272, -0.0135], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact there are an awful lot of parameters: $(784*512)+ 512+(512*512)+512+(512*10)+10 =669,706$. Almost $700,000$ parameters. The number is a result of the model architecture we defined as we defined the `NeuralNetwork` class. Let us look at a graphical representation of what we build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an image that is represented by 784 pieces of information, say $X_i$ for $i = 1,..., 784. (#0)\n",
    "\n",
    "Each of the units in the first hidden layer (#1) represents a linear combination of these 784 pieces of information into a unit in the first hidden layer, think, $Lin^{(1)}_k=\\sum^{784}_{i=1}\\omega^{(1)}_{ki}X_i$ for the $k$ th of these units. That means that into each of these units we have 784 values and a parameter for each, and then you find 512 units. \n",
    "\n",
    "Each of these units you can think of as a function which combines the linear combination of all the image information ($Lin^{(1)}_k$) into an output $Lout^{(1)}_k=g(\\omega^{(1)}_{k0}+Lin^{(1)}_k)$. This requires another parameter for each unit $k$, $\\omega^{(1)}_{k0}$ and a function $g()$. In the setup we set this function to be a rectified linear activation function (`nn.ReLU()`). This is an extremely simple function of the form $g(x)=max(0,x)$.\n",
    "\n",
    "The 2nd hidden layer (#2) functions very much in the same manner. All inputs (= all outputs from the previous layer) are linearily combined into the input for the $k$th unit in the second layer, $Lin^{(2)}_k=\\sum^{512}_{i=1}\\omega^{(2)}_{ki}Lout^{(1)}_i$. This implies that there are $512*512$ parameters ($\\omega^{(2)}_{ki}$) at this stage. Again, as we specified in our class definition, the function that combines these into an output for unit $k$ is `nn.ReLU()`, $Lout^{(2)}_k=g(\\omega^{(2)}_{k0}+Lin^{(2)}_k)$. This requires 512 additional parameters, $\\omega^{(2)}_{k0}$.\n",
    "\n",
    "Finally, layer #3 defines the model outputs. Here we have 10, as specified in the class setup (`nn.Linear(512, 10)`). Each of these is a linear combination of the 512 outputs from layer 2, $Y_k=\\sum^{512}_{i=1}\\omega^{(o)}_{ki}Lout^{(2)}_i$. As we have 10 such outputs, this delivers $10*512$ additional parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeuralNetwork Structure](images/NNstructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have almost $700,000$ parameters and initially they are all chosen to take some random values. We shouldn't be surprised that the model was unable to distinguish an ankle boot from a t-shirt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate/Optimise/Train the model\n",
    "\n",
    "We now turn attention to the model optimisation. An econometrician would call this parameter estimation, in data science this is typically called the training o fthe model.\n",
    "\n",
    "Recall that earlier, we initiated an instance of our neural network, `model`, by calling the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any code after that line was just for illustration purposes of the networks working. So let's go back to the stage where we just initiated `model`. As discussed before the model will have been initiated with some random parameters.\n",
    "\n",
    "In order to find the best parameters that allow the model to differentiate an ankle boot from a t-shirt we will need to feed the network some data, we call this this the training data set, `ds_train`, for which we know what categories of items they are. The parameters are adjusted to keep improving the model's categorisation of these data. This is a nonlinear optimisation process, in other words it is an iterative process which potentially never ends. \n",
    "\n",
    "To make this a feasible process we have to give Python (or better `torch`) a few instructions for this process. These instructions come in the form of what is commonly called hyperparameters. Typical hyperparameters are\n",
    "\n",
    "* Number of epochs, or number of parameter improvement iterations \n",
    "* Batch size, this tells torch how many pictures to feed into the model in order to find the next improvement\n",
    "* Learning rate, this instructs torch how quickly to update parameters\n",
    "\n",
    "We adopt the same values chosen in the Torch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing these differently will give you slightly different results. However, in the end, the idea is that these should be chosen such that the results are not sensitive to sensible changes in tehse parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some further choices to be made, namely the loss function and the algorithm.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "As econometricians we are familiar with loss functions like the residual sum of squares (which we aim to minimise) or the (log-)likelihood function (which we aim to maximise). Both of these are available to users. Here however we use a different loss function called the `nn.CrossEntropyLoss`. In effect we are comparing two discrete distributions. The real outcome, say of our test ankle boot is a 1x10 vector of values with all but the last element being a 0, the last being equal to 1. Think of this as a discrete distribution.\n",
    "\n",
    "As you have seen, the output of the neural network we build is also a 1x10 vector of probabilities. Ideally we wish that the network predicts the right label (high probability on ankle boot and close to 0 probability for the other labels). Therefore, the closer the output vector is to the input label vector, the better our model does. \n",
    "\n",
    "The `nn.CrossEntropyLoss` loss function can be used to compare such vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation algorithm\n",
    "\n",
    "The process of updating the parameters (`model.parameters()`) to improve the model is an iterative one and we will have to ask `torch` to use one of the algorithms implemented in the package. Below we are using `torch.optim` and in particular we use the stochastic gradient descent method (`.SGD`). Check out the [torch.optim documentation](https://pytorch.org/docs/stable/optim.html) to see a list of the implemented algorithms. Algorithms need some parameters setting and this one needs a learning rate, one of the hyperparameters listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimising parameters in a neural network, there is always the challenge of avoiding over-fitting. As discussed before, this \"simple\" neural network is still a highly complex and nonlinear model with almost 700,000 parameters. If left unconstrained, we would be able to achieve an almost perfect fit for our set of 60,000 images in the training dataset. But that is not really the challenge at hand. The challenge is to correctly categorise unseen images. This is why 10,000 images have been put aside in the test dataset.\n",
    "\n",
    "This challenge requires a careful interplay between finding the parameters that correctly categorise the data which are presented to the network (with known categories/labels), but at the same time ensuring that the model does not specialise too much on the presented set of images. This is done by evaluating when improvements start to only improve the fit in the training dataset but do not any longer improve the predictions in the test dataset.\n",
    "\n",
    "In order to achieve this we need to define the following two functions. First, a training step (`train_loop`) which uses the 60,000 images in the dataset (and their correct labels) to find improvements to teh parameter values. Second, an evaluation/test step (`test_loop`). This evaluation step checks how well the model with the updated parameters from the training step, predict the labels of the 10,000 images in the test dataset. This step does not change the network parameters. We will be iterating between the two. We should expect the training step to always lead to improvements in the fit of the training dataset. In terms of predicting the training dataset we should initially also expect improvements, but eventually we shoudl expect no further improvements. This is the moment when the network begins to over-fit to the data in the training dataset. This is is when we can stop the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    print(f\"Training:  Avg loss: {train_loss:>8f} \\n\")\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the functions for the training and evaluation step are written we can just call them iteratively. Each training step uses all 60,000 data (in batches of 64) to optimise the parameters and then each evaluation step uses all 10,000 images to test the quality of the model.\n",
    "\n",
    "As we go through this loop we save the loss functions (`save_loss`) for the training and test data such that we can afterwards look at their development as we cycle through each of these steps (called epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298532  [   64/60000]\n",
      "loss: 2.289743  [ 6464/60000]\n",
      "loss: 2.279294  [12864/60000]\n",
      "loss: 2.255660  [19264/60000]\n",
      "loss: 2.243678  [25664/60000]\n",
      "loss: 2.220732  [32064/60000]\n",
      "loss: 2.227350  [38464/60000]\n",
      "loss: 2.205025  [44864/60000]\n",
      "loss: 2.170919  [51264/60000]\n",
      "loss: 2.127726  [57664/60000]\n",
      "Training:  Avg loss: 2.227983 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.142737 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.184294  [   64/60000]\n",
      "loss: 2.113731  [ 6464/60000]\n",
      "loss: 2.074864  [12864/60000]\n",
      "loss: 2.058708  [19264/60000]\n",
      "loss: 2.062468  [25664/60000]\n",
      "loss: 2.000256  [32064/60000]\n",
      "loss: 2.026388  [38464/60000]\n",
      "loss: 1.957521  [44864/60000]\n",
      "loss: 1.910275  [51264/60000]\n",
      "loss: 1.934209  [57664/60000]\n",
      "Training:  Avg loss: 2.013925 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.859512 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.919831  [   64/60000]\n",
      "loss: 1.786309  [ 6464/60000]\n",
      "loss: 1.852429  [12864/60000]\n",
      "loss: 1.749505  [19264/60000]\n",
      "loss: 1.697453  [25664/60000]\n",
      "loss: 1.666074  [32064/60000]\n",
      "loss: 1.612206  [38464/60000]\n",
      "loss: 1.550460  [44864/60000]\n",
      "loss: 1.616006  [51264/60000]\n",
      "loss: 1.549967  [57664/60000]\n",
      "Training:  Avg loss: 1.671699 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.500091 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.448203  [   64/60000]\n",
      "loss: 1.470392  [ 6464/60000]\n",
      "loss: 1.566669  [12864/60000]\n",
      "loss: 1.351499  [19264/60000]\n",
      "loss: 1.343482  [25664/60000]\n",
      "loss: 1.326560  [32064/60000]\n",
      "loss: 1.425466  [38464/60000]\n",
      "loss: 1.354166  [44864/60000]\n",
      "loss: 1.253878  [51264/60000]\n",
      "loss: 1.235223  [57664/60000]\n",
      "Training:  Avg loss: 1.360737 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 1.252826 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.284077  [   64/60000]\n",
      "loss: 1.165842  [ 6464/60000]\n",
      "loss: 1.153011  [12864/60000]\n",
      "loss: 1.158080  [19264/60000]\n",
      "loss: 1.219942  [25664/60000]\n",
      "loss: 1.183282  [32064/60000]\n",
      "loss: 1.185815  [38464/60000]\n",
      "loss: 1.162471  [44864/60000]\n",
      "loss: 1.002640  [51264/60000]\n",
      "loss: 1.086006  [57664/60000]\n",
      "Training:  Avg loss: 1.160068 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.093596 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.100469  [   64/60000]\n",
      "loss: 1.107111  [ 6464/60000]\n",
      "loss: 0.940219  [12864/60000]\n",
      "loss: 1.203690  [19264/60000]\n",
      "loss: 0.938860  [25664/60000]\n",
      "loss: 1.019288  [32064/60000]\n",
      "loss: 1.031710  [38464/60000]\n",
      "loss: 0.931231  [44864/60000]\n",
      "loss: 0.859177  [51264/60000]\n",
      "loss: 0.971455  [57664/60000]\n",
      "Training:  Avg loss: 1.028146 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.988706 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.945242  [   64/60000]\n",
      "loss: 0.996501  [ 6464/60000]\n",
      "loss: 1.035764  [12864/60000]\n",
      "loss: 0.833000  [19264/60000]\n",
      "loss: 0.803923  [25664/60000]\n",
      "loss: 1.062509  [32064/60000]\n",
      "loss: 0.816259  [38464/60000]\n",
      "loss: 0.831740  [44864/60000]\n",
      "loss: 0.950867  [51264/60000]\n",
      "loss: 0.904064  [57664/60000]\n",
      "Training:  Avg loss: 0.937991 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.913812 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.861781  [   64/60000]\n",
      "loss: 0.894671  [ 6464/60000]\n",
      "loss: 0.964126  [12864/60000]\n",
      "loss: 1.015488  [19264/60000]\n",
      "loss: 0.905006  [25664/60000]\n",
      "loss: 0.687003  [32064/60000]\n",
      "loss: 0.753945  [38464/60000]\n",
      "loss: 0.894196  [44864/60000]\n",
      "loss: 0.842911  [51264/60000]\n",
      "loss: 0.869812  [57664/60000]\n",
      "Training:  Avg loss: 0.874258 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.862455 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.796063  [   64/60000]\n",
      "loss: 1.076437  [ 6464/60000]\n",
      "loss: 0.857867  [12864/60000]\n",
      "loss: 0.793358  [19264/60000]\n",
      "loss: 0.918363  [25664/60000]\n",
      "loss: 0.676771  [32064/60000]\n",
      "loss: 0.894671  [38464/60000]\n",
      "loss: 0.834851  [44864/60000]\n",
      "loss: 0.849899  [51264/60000]\n",
      "loss: 0.731081  [57664/60000]\n",
      "Training:  Avg loss: 0.827677 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.822487 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.949339  [   64/60000]\n",
      "loss: 0.584500  [ 6464/60000]\n",
      "loss: 0.779696  [12864/60000]\n",
      "loss: 0.688902  [19264/60000]\n",
      "loss: 0.768254  [25664/60000]\n",
      "loss: 0.749628  [32064/60000]\n",
      "loss: 0.795666  [38464/60000]\n",
      "loss: 0.801303  [44864/60000]\n",
      "loss: 0.614506  [51264/60000]\n",
      "loss: 0.960770  [57664/60000]\n",
      "Training:  Avg loss: 0.791865 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.791772 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.623366  [   64/60000]\n",
      "loss: 1.030589  [ 6464/60000]\n",
      "loss: 0.815113  [12864/60000]\n",
      "loss: 0.908489  [19264/60000]\n",
      "loss: 0.735603  [25664/60000]\n",
      "loss: 0.675868  [32064/60000]\n",
      "loss: 0.982758  [38464/60000]\n",
      "loss: 0.628550  [44864/60000]\n",
      "loss: 0.683309  [51264/60000]\n",
      "loss: 0.876658  [57664/60000]\n",
      "Training:  Avg loss: 0.762631 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.764867 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.797179  [   64/60000]\n",
      "loss: 0.733122  [ 6464/60000]\n",
      "loss: 0.758931  [12864/60000]\n",
      "loss: 0.735790  [19264/60000]\n",
      "loss: 0.665153  [25664/60000]\n",
      "loss: 0.811129  [32064/60000]\n",
      "loss: 0.691191  [38464/60000]\n",
      "loss: 0.762898  [44864/60000]\n",
      "loss: 0.880220  [51264/60000]\n",
      "loss: 0.779716  [57664/60000]\n",
      "Training:  Avg loss: 0.738212 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.742777 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.670238  [   64/60000]\n",
      "loss: 0.664685  [ 6464/60000]\n",
      "loss: 0.584924  [12864/60000]\n",
      "loss: 0.732356  [19264/60000]\n",
      "loss: 0.727411  [25664/60000]\n",
      "loss: 0.721036  [32064/60000]\n",
      "loss: 0.897042  [38464/60000]\n",
      "loss: 0.732634  [44864/60000]\n",
      "loss: 0.682275  [51264/60000]\n",
      "loss: 0.817855  [57664/60000]\n",
      "Training:  Avg loss: 0.716655 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.721524 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.785921  [   64/60000]\n",
      "loss: 0.809043  [ 6464/60000]\n",
      "loss: 0.564047  [12864/60000]\n",
      "loss: 0.744361  [19264/60000]\n",
      "loss: 0.576329  [25664/60000]\n",
      "loss: 0.690446  [32064/60000]\n",
      "loss: 0.699509  [38464/60000]\n",
      "loss: 0.725427  [44864/60000]\n",
      "loss: 0.629143  [51264/60000]\n",
      "loss: 0.590732  [57664/60000]\n",
      "Training:  Avg loss: 0.697290 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.704287 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.762699  [   64/60000]\n",
      "loss: 0.662101  [ 6464/60000]\n",
      "loss: 0.663817  [12864/60000]\n",
      "loss: 0.648505  [19264/60000]\n",
      "loss: 0.720434  [25664/60000]\n",
      "loss: 0.652357  [32064/60000]\n",
      "loss: 0.632540  [38464/60000]\n",
      "loss: 0.738535  [44864/60000]\n",
      "loss: 0.492663  [51264/60000]\n",
      "loss: 0.644983  [57664/60000]\n",
      "Training:  Avg loss: 0.679345 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.690431 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.568191  [   64/60000]\n",
      "loss: 0.714668  [ 6464/60000]\n",
      "loss: 0.689558  [12864/60000]\n",
      "loss: 0.669038  [19264/60000]\n",
      "loss: 0.708230  [25664/60000]\n",
      "loss: 0.618782  [32064/60000]\n",
      "loss: 0.673686  [38464/60000]\n",
      "loss: 0.601638  [44864/60000]\n",
      "loss: 0.487470  [51264/60000]\n",
      "loss: 0.642209  [57664/60000]\n",
      "Training:  Avg loss: 0.663292 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.673799 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.592583  [   64/60000]\n",
      "loss: 0.755441  [ 6464/60000]\n",
      "loss: 0.739926  [12864/60000]\n",
      "loss: 0.677378  [19264/60000]\n",
      "loss: 0.654386  [25664/60000]\n",
      "loss: 0.662438  [32064/60000]\n",
      "loss: 0.762327  [38464/60000]\n",
      "loss: 0.623549  [44864/60000]\n",
      "loss: 0.599485  [51264/60000]\n",
      "loss: 0.622419  [57664/60000]\n",
      "Training:  Avg loss: 0.648064 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.663109 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.615334  [   64/60000]\n",
      "loss: 0.542004  [ 6464/60000]\n",
      "loss: 0.679787  [12864/60000]\n",
      "loss: 0.725866  [19264/60000]\n",
      "loss: 0.605290  [25664/60000]\n",
      "loss: 0.566916  [32064/60000]\n",
      "loss: 0.662562  [38464/60000]\n",
      "loss: 0.631135  [44864/60000]\n",
      "loss: 0.524498  [51264/60000]\n",
      "loss: 0.555956  [57664/60000]\n",
      "Training:  Avg loss: 0.633753 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.648938 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.667491  [   64/60000]\n",
      "loss: 0.515058  [ 6464/60000]\n",
      "loss: 0.520290  [12864/60000]\n",
      "loss: 0.552431  [19264/60000]\n",
      "loss: 0.622699  [25664/60000]\n",
      "loss: 0.676777  [32064/60000]\n",
      "loss: 0.538169  [38464/60000]\n",
      "loss: 0.464249  [44864/60000]\n",
      "loss: 0.553061  [51264/60000]\n",
      "loss: 0.536799  [57664/60000]\n",
      "Training:  Avg loss: 0.620876 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.632585 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.628431  [   64/60000]\n",
      "loss: 0.499238  [ 6464/60000]\n",
      "loss: 0.683474  [12864/60000]\n",
      "loss: 0.688266  [19264/60000]\n",
      "loss: 0.617458  [25664/60000]\n",
      "loss: 0.495780  [32064/60000]\n",
      "loss: 0.789429  [38464/60000]\n",
      "loss: 0.589537  [44864/60000]\n",
      "loss: 0.515494  [51264/60000]\n",
      "loss: 0.497122  [57664/60000]\n",
      "Training:  Avg loss: 0.608695 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.621169 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.542238  [   64/60000]\n",
      "loss: 0.653538  [ 6464/60000]\n",
      "loss: 0.793532  [12864/60000]\n",
      "loss: 0.670286  [19264/60000]\n",
      "loss: 0.741615  [25664/60000]\n",
      "loss: 0.522330  [32064/60000]\n",
      "loss: 0.440280  [38464/60000]\n",
      "loss: 0.611068  [44864/60000]\n",
      "loss: 0.561672  [51264/60000]\n",
      "loss: 0.556120  [57664/60000]\n",
      "Training:  Avg loss: 0.597504 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.611513 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.467702  [   64/60000]\n",
      "loss: 0.451842  [ 6464/60000]\n",
      "loss: 0.723985  [12864/60000]\n",
      "loss: 0.591683  [19264/60000]\n",
      "loss: 0.648059  [25664/60000]\n",
      "loss: 0.474802  [32064/60000]\n",
      "loss: 0.653512  [38464/60000]\n",
      "loss: 0.492108  [44864/60000]\n",
      "loss: 0.701906  [51264/60000]\n",
      "loss: 0.589475  [57664/60000]\n",
      "Training:  Avg loss: 0.587255 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.602350 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.594063  [   64/60000]\n",
      "loss: 0.628489  [ 6464/60000]\n",
      "loss: 0.325032  [12864/60000]\n",
      "loss: 0.714758  [19264/60000]\n",
      "loss: 0.507848  [25664/60000]\n",
      "loss: 0.672610  [32064/60000]\n",
      "loss: 0.491266  [38464/60000]\n",
      "loss: 0.460033  [44864/60000]\n",
      "loss: 0.619063  [51264/60000]\n",
      "loss: 0.438546  [57664/60000]\n",
      "Training:  Avg loss: 0.577481 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.592881 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.607856  [   64/60000]\n",
      "loss: 0.500895  [ 6464/60000]\n",
      "loss: 0.703012  [12864/60000]\n",
      "loss: 0.613659  [19264/60000]\n",
      "loss: 0.531148  [25664/60000]\n",
      "loss: 0.441333  [32064/60000]\n",
      "loss: 0.621263  [38464/60000]\n",
      "loss: 0.667070  [44864/60000]\n",
      "loss: 0.396283  [51264/60000]\n",
      "loss: 0.592403  [57664/60000]\n",
      "Training:  Avg loss: 0.568517 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.586478 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.648326  [   64/60000]\n",
      "loss: 0.587054  [ 6464/60000]\n",
      "loss: 0.499688  [12864/60000]\n",
      "loss: 0.671928  [19264/60000]\n",
      "loss: 0.516193  [25664/60000]\n",
      "loss: 0.472166  [32064/60000]\n",
      "loss: 0.532750  [38464/60000]\n",
      "loss: 0.487506  [44864/60000]\n",
      "loss: 0.506655  [51264/60000]\n",
      "loss: 0.439543  [57664/60000]\n",
      "Training:  Avg loss: 0.560280 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.579949 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.549302  [   64/60000]\n",
      "loss: 0.507599  [ 6464/60000]\n",
      "loss: 0.534281  [12864/60000]\n",
      "loss: 0.710572  [19264/60000]\n",
      "loss: 0.429861  [25664/60000]\n",
      "loss: 0.596533  [32064/60000]\n",
      "loss: 0.437091  [38464/60000]\n",
      "loss: 0.431361  [44864/60000]\n",
      "loss: 0.541147  [51264/60000]\n",
      "loss: 0.729079  [57664/60000]\n",
      "Training:  Avg loss: 0.552573 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.572596 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.511283  [   64/60000]\n",
      "loss: 0.517016  [ 6464/60000]\n",
      "loss: 0.612907  [12864/60000]\n",
      "loss: 0.410109  [19264/60000]\n",
      "loss: 0.543420  [25664/60000]\n",
      "loss: 0.659044  [32064/60000]\n",
      "loss: 0.544553  [38464/60000]\n",
      "loss: 0.539921  [44864/60000]\n",
      "loss: 0.761254  [51264/60000]\n",
      "loss: 0.406782  [57664/60000]\n",
      "Training:  Avg loss: 0.545576 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.565942 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.627768  [   64/60000]\n",
      "loss: 0.697849  [ 6464/60000]\n",
      "loss: 0.534019  [12864/60000]\n",
      "loss: 0.514086  [19264/60000]\n",
      "loss: 0.524095  [25664/60000]\n",
      "loss: 0.519459  [32064/60000]\n",
      "loss: 0.481594  [38464/60000]\n",
      "loss: 0.460370  [44864/60000]\n",
      "loss: 0.487014  [51264/60000]\n",
      "loss: 0.526797  [57664/60000]\n",
      "Training:  Avg loss: 0.538868 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.560496 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.389162  [   64/60000]\n",
      "loss: 0.533802  [ 6464/60000]\n",
      "loss: 0.578550  [12864/60000]\n",
      "loss: 0.500618  [19264/60000]\n",
      "loss: 0.604662  [25664/60000]\n",
      "loss: 0.663092  [32064/60000]\n",
      "loss: 0.373549  [38464/60000]\n",
      "loss: 0.457558  [44864/60000]\n",
      "loss: 0.519003  [51264/60000]\n",
      "loss: 0.471867  [57664/60000]\n",
      "Training:  Avg loss: 0.532741 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.553829 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.627958  [   64/60000]\n",
      "loss: 0.603418  [ 6464/60000]\n",
      "loss: 0.596318  [12864/60000]\n",
      "loss: 0.597319  [19264/60000]\n",
      "loss: 0.520079  [25664/60000]\n",
      "loss: 0.385438  [32064/60000]\n",
      "loss: 0.540811  [38464/60000]\n",
      "loss: 0.435725  [44864/60000]\n",
      "loss: 0.422362  [51264/60000]\n",
      "loss: 0.443935  [57664/60000]\n",
      "Training:  Avg loss: 0.526691 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.548262 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.444135  [   64/60000]\n",
      "loss: 0.665835  [ 6464/60000]\n",
      "loss: 0.660750  [12864/60000]\n",
      "loss: 0.578654  [19264/60000]\n",
      "loss: 0.473765  [25664/60000]\n",
      "loss: 0.414590  [32064/60000]\n",
      "loss: 0.495708  [38464/60000]\n",
      "loss: 0.584465  [44864/60000]\n",
      "loss: 0.541048  [51264/60000]\n",
      "loss: 0.583207  [57664/60000]\n",
      "Training:  Avg loss: 0.521531 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.544556 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.533246  [   64/60000]\n",
      "loss: 0.522221  [ 6464/60000]\n",
      "loss: 0.505837  [12864/60000]\n",
      "loss: 0.542025  [19264/60000]\n",
      "loss: 0.648831  [25664/60000]\n",
      "loss: 0.550267  [32064/60000]\n",
      "loss: 0.511932  [38464/60000]\n",
      "loss: 0.540213  [44864/60000]\n",
      "loss: 0.485319  [51264/60000]\n",
      "loss: 0.562913  [57664/60000]\n",
      "Training:  Avg loss: 0.516483 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.539831 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.348385  [   64/60000]\n",
      "loss: 0.738395  [ 6464/60000]\n",
      "loss: 0.603781  [12864/60000]\n",
      "loss: 0.471077  [19264/60000]\n",
      "loss: 0.521733  [25664/60000]\n",
      "loss: 0.523441  [32064/60000]\n",
      "loss: 0.655108  [38464/60000]\n",
      "loss: 0.439272  [44864/60000]\n",
      "loss: 0.352483  [51264/60000]\n",
      "loss: 0.681160  [57664/60000]\n",
      "Training:  Avg loss: 0.511783 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.534943 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.398766  [   64/60000]\n",
      "loss: 0.655237  [ 6464/60000]\n",
      "loss: 0.430160  [12864/60000]\n",
      "loss: 0.828920  [19264/60000]\n",
      "loss: 0.439598  [25664/60000]\n",
      "loss: 0.454108  [32064/60000]\n",
      "loss: 0.557259  [38464/60000]\n",
      "loss: 0.561691  [44864/60000]\n",
      "loss: 0.516948  [51264/60000]\n",
      "loss: 0.468143  [57664/60000]\n",
      "Training:  Avg loss: 0.507250 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.531075 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.591575  [   64/60000]\n",
      "loss: 0.561601  [ 6464/60000]\n",
      "loss: 0.345327  [12864/60000]\n",
      "loss: 0.424655  [19264/60000]\n",
      "loss: 0.427621  [25664/60000]\n",
      "loss: 0.398327  [32064/60000]\n",
      "loss: 0.667116  [38464/60000]\n",
      "loss: 0.402564  [44864/60000]\n",
      "loss: 0.474849  [51264/60000]\n",
      "loss: 0.749186  [57664/60000]\n",
      "Training:  Avg loss: 0.503095 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.530257 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.456398  [   64/60000]\n",
      "loss: 0.578338  [ 6464/60000]\n",
      "loss: 0.634156  [12864/60000]\n",
      "loss: 0.636776  [19264/60000]\n",
      "loss: 0.479900  [25664/60000]\n",
      "loss: 0.511273  [32064/60000]\n",
      "loss: 0.545797  [38464/60000]\n",
      "loss: 0.534171  [44864/60000]\n",
      "loss: 0.375296  [51264/60000]\n",
      "loss: 0.579457  [57664/60000]\n",
      "Training:  Avg loss: 0.499380 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.525450 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.440079  [   64/60000]\n",
      "loss: 0.596023  [ 6464/60000]\n",
      "loss: 0.588442  [12864/60000]\n",
      "loss: 0.682499  [19264/60000]\n",
      "loss: 0.702867  [25664/60000]\n",
      "loss: 0.526552  [32064/60000]\n",
      "loss: 0.503294  [38464/60000]\n",
      "loss: 0.366795  [44864/60000]\n",
      "loss: 0.398253  [51264/60000]\n",
      "loss: 0.446442  [57664/60000]\n",
      "Training:  Avg loss: 0.495437 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.521937 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.445099  [   64/60000]\n",
      "loss: 0.453704  [ 6464/60000]\n",
      "loss: 0.472722  [12864/60000]\n",
      "loss: 0.620473  [19264/60000]\n",
      "loss: 0.367575  [25664/60000]\n",
      "loss: 0.620017  [32064/60000]\n",
      "loss: 0.587338  [38464/60000]\n",
      "loss: 0.661958  [44864/60000]\n",
      "loss: 0.509145  [51264/60000]\n",
      "loss: 0.437891  [57664/60000]\n",
      "Training:  Avg loss: 0.491954 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.519837 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.651867  [   64/60000]\n",
      "loss: 0.555610  [ 6464/60000]\n",
      "loss: 0.513715  [12864/60000]\n",
      "loss: 0.281187  [19264/60000]\n",
      "loss: 0.449058  [25664/60000]\n",
      "loss: 0.462167  [32064/60000]\n",
      "loss: 0.666167  [38464/60000]\n",
      "loss: 0.471808  [44864/60000]\n",
      "loss: 0.580069  [51264/60000]\n",
      "loss: 0.506355  [57664/60000]\n",
      "Training:  Avg loss: 0.488865 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.516621 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.427972  [   64/60000]\n",
      "loss: 0.487687  [ 6464/60000]\n",
      "loss: 0.435091  [12864/60000]\n",
      "loss: 0.645026  [19264/60000]\n",
      "loss: 0.442070  [25664/60000]\n",
      "loss: 0.415334  [32064/60000]\n",
      "loss: 0.559285  [38464/60000]\n",
      "loss: 0.344563  [44864/60000]\n",
      "loss: 0.375805  [51264/60000]\n",
      "loss: 0.619541  [57664/60000]\n",
      "Training:  Avg loss: 0.485418 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.513849 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.320613  [   64/60000]\n",
      "loss: 0.589153  [ 6464/60000]\n",
      "loss: 0.377996  [12864/60000]\n",
      "loss: 0.411964  [19264/60000]\n",
      "loss: 0.376281  [25664/60000]\n",
      "loss: 0.383596  [32064/60000]\n",
      "loss: 0.512387  [38464/60000]\n",
      "loss: 0.509902  [44864/60000]\n",
      "loss: 0.531494  [51264/60000]\n",
      "loss: 0.379005  [57664/60000]\n",
      "Training:  Avg loss: 0.482343 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.511641 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.516396  [   64/60000]\n",
      "loss: 0.319893  [ 6464/60000]\n",
      "loss: 0.408751  [12864/60000]\n",
      "loss: 0.726786  [19264/60000]\n",
      "loss: 0.403084  [25664/60000]\n",
      "loss: 0.539679  [32064/60000]\n",
      "loss: 0.464528  [38464/60000]\n",
      "loss: 0.514418  [44864/60000]\n",
      "loss: 0.452978  [51264/60000]\n",
      "loss: 0.494505  [57664/60000]\n",
      "Training:  Avg loss: 0.479758 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.507009 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.404416  [   64/60000]\n",
      "loss: 0.237183  [ 6464/60000]\n",
      "loss: 0.379559  [12864/60000]\n",
      "loss: 0.736608  [19264/60000]\n",
      "loss: 0.420855  [25664/60000]\n",
      "loss: 0.515755  [32064/60000]\n",
      "loss: 0.523773  [38464/60000]\n",
      "loss: 0.458461  [44864/60000]\n",
      "loss: 0.466719  [51264/60000]\n",
      "loss: 0.300011  [57664/60000]\n",
      "Training:  Avg loss: 0.476984 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.505434 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.295450  [   64/60000]\n",
      "loss: 0.462097  [ 6464/60000]\n",
      "loss: 0.679482  [12864/60000]\n",
      "loss: 0.481777  [19264/60000]\n",
      "loss: 0.490383  [25664/60000]\n",
      "loss: 0.422894  [32064/60000]\n",
      "loss: 0.378107  [38464/60000]\n",
      "loss: 0.440765  [44864/60000]\n",
      "loss: 0.444241  [51264/60000]\n",
      "loss: 0.459480  [57664/60000]\n",
      "Training:  Avg loss: 0.474526 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.504213 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.659779  [   64/60000]\n",
      "loss: 0.482760  [ 6464/60000]\n",
      "loss: 0.508350  [12864/60000]\n",
      "loss: 0.501682  [19264/60000]\n",
      "loss: 0.449626  [25664/60000]\n",
      "loss: 0.449312  [32064/60000]\n",
      "loss: 0.350504  [38464/60000]\n",
      "loss: 0.428965  [44864/60000]\n",
      "loss: 0.448845  [51264/60000]\n",
      "loss: 0.412556  [57664/60000]\n",
      "Training:  Avg loss: 0.471911 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.501164 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.393261  [   64/60000]\n",
      "loss: 0.397639  [ 6464/60000]\n",
      "loss: 0.398137  [12864/60000]\n",
      "loss: 0.549782  [19264/60000]\n",
      "loss: 0.474174  [25664/60000]\n",
      "loss: 0.464824  [32064/60000]\n",
      "loss: 0.346024  [38464/60000]\n",
      "loss: 0.481695  [44864/60000]\n",
      "loss: 0.380709  [51264/60000]\n",
      "loss: 0.395363  [57664/60000]\n",
      "Training:  Avg loss: 0.469399 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.496697 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.606119  [   64/60000]\n",
      "loss: 0.538122  [ 6464/60000]\n",
      "loss: 0.452793  [12864/60000]\n",
      "loss: 0.457637  [19264/60000]\n",
      "loss: 0.455114  [25664/60000]\n",
      "loss: 0.362156  [32064/60000]\n",
      "loss: 0.393460  [38464/60000]\n",
      "loss: 0.275612  [44864/60000]\n",
      "loss: 0.461445  [51264/60000]\n",
      "loss: 0.367144  [57664/60000]\n",
      "Training:  Avg loss: 0.466973 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.496276 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.609811  [   64/60000]\n",
      "loss: 0.359059  [ 6464/60000]\n",
      "loss: 0.588134  [12864/60000]\n",
      "loss: 0.434405  [19264/60000]\n",
      "loss: 0.414817  [25664/60000]\n",
      "loss: 0.406991  [32064/60000]\n",
      "loss: 0.507965  [38464/60000]\n",
      "loss: 0.488370  [44864/60000]\n",
      "loss: 0.576937  [51264/60000]\n",
      "loss: 0.357425  [57664/60000]\n",
      "Training:  Avg loss: 0.464900 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.493302 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.338523  [   64/60000]\n",
      "loss: 0.520876  [ 6464/60000]\n",
      "loss: 0.529586  [12864/60000]\n",
      "loss: 0.407384  [19264/60000]\n",
      "loss: 0.413046  [25664/60000]\n",
      "loss: 0.648664  [32064/60000]\n",
      "loss: 0.538430  [38464/60000]\n",
      "loss: 0.408102  [44864/60000]\n",
      "loss: 0.504493  [51264/60000]\n",
      "loss: 0.431288  [57664/60000]\n",
      "Training:  Avg loss: 0.462886 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.490710 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.400128  [   64/60000]\n",
      "loss: 0.535007  [ 6464/60000]\n",
      "loss: 0.447540  [12864/60000]\n",
      "loss: 0.436445  [19264/60000]\n",
      "loss: 0.533107  [25664/60000]\n",
      "loss: 0.246256  [32064/60000]\n",
      "loss: 0.365048  [38464/60000]\n",
      "loss: 0.545772  [44864/60000]\n",
      "loss: 0.289432  [51264/60000]\n",
      "loss: 0.509429  [57664/60000]\n",
      "Training:  Avg loss: 0.460647 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.491504 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.474120  [   64/60000]\n",
      "loss: 0.463045  [ 6464/60000]\n",
      "loss: 0.489152  [12864/60000]\n",
      "loss: 0.367888  [19264/60000]\n",
      "loss: 0.311571  [25664/60000]\n",
      "loss: 0.455630  [32064/60000]\n",
      "loss: 0.509313  [38464/60000]\n",
      "loss: 0.624610  [44864/60000]\n",
      "loss: 0.414112  [51264/60000]\n",
      "loss: 0.385604  [57664/60000]\n",
      "Training:  Avg loss: 0.458892 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.488112 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.410993  [   64/60000]\n",
      "loss: 0.604121  [ 6464/60000]\n",
      "loss: 0.537265  [12864/60000]\n",
      "loss: 0.470522  [19264/60000]\n",
      "loss: 0.366119  [25664/60000]\n",
      "loss: 0.475463  [32064/60000]\n",
      "loss: 0.250416  [38464/60000]\n",
      "loss: 0.441595  [44864/60000]\n",
      "loss: 0.585680  [51264/60000]\n",
      "loss: 0.331575  [57664/60000]\n",
      "Training:  Avg loss: 0.456991 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.487714 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.381970  [   64/60000]\n",
      "loss: 0.456165  [ 6464/60000]\n",
      "loss: 0.459149  [12864/60000]\n",
      "loss: 0.482384  [19264/60000]\n",
      "loss: 0.313679  [25664/60000]\n",
      "loss: 0.544400  [32064/60000]\n",
      "loss: 0.400850  [38464/60000]\n",
      "loss: 0.655654  [44864/60000]\n",
      "loss: 0.364910  [51264/60000]\n",
      "loss: 0.436099  [57664/60000]\n",
      "Training:  Avg loss: 0.454896 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.484523 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.384494  [   64/60000]\n",
      "loss: 0.286357  [ 6464/60000]\n",
      "loss: 0.233997  [12864/60000]\n",
      "loss: 0.491319  [19264/60000]\n",
      "loss: 0.423309  [25664/60000]\n",
      "loss: 0.453871  [32064/60000]\n",
      "loss: 0.360182  [38464/60000]\n",
      "loss: 0.417257  [44864/60000]\n",
      "loss: 0.396173  [51264/60000]\n",
      "loss: 0.481867  [57664/60000]\n",
      "Training:  Avg loss: 0.453238 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.484046 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.352876  [   64/60000]\n",
      "loss: 0.515044  [ 6464/60000]\n",
      "loss: 0.706136  [12864/60000]\n",
      "loss: 0.391102  [19264/60000]\n",
      "loss: 0.521491  [25664/60000]\n",
      "loss: 0.490059  [32064/60000]\n",
      "loss: 0.209970  [38464/60000]\n",
      "loss: 0.417378  [44864/60000]\n",
      "loss: 0.435987  [51264/60000]\n",
      "loss: 0.363384  [57664/60000]\n",
      "Training:  Avg loss: 0.451396 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.481934 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.340688  [   64/60000]\n",
      "loss: 0.418613  [ 6464/60000]\n",
      "loss: 0.497102  [12864/60000]\n",
      "loss: 0.362921  [19264/60000]\n",
      "loss: 0.444783  [25664/60000]\n",
      "loss: 0.387098  [32064/60000]\n",
      "loss: 0.372031  [38464/60000]\n",
      "loss: 0.612548  [44864/60000]\n",
      "loss: 0.347098  [51264/60000]\n",
      "loss: 0.474584  [57664/60000]\n",
      "Training:  Avg loss: 0.449584 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.481721 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.424630  [   64/60000]\n",
      "loss: 0.469763  [ 6464/60000]\n",
      "loss: 0.512142  [12864/60000]\n",
      "loss: 0.434293  [19264/60000]\n",
      "loss: 0.493829  [25664/60000]\n",
      "loss: 0.443271  [32064/60000]\n",
      "loss: 0.380195  [38464/60000]\n",
      "loss: 0.380479  [44864/60000]\n",
      "loss: 0.395431  [51264/60000]\n",
      "loss: 0.426420  [57664/60000]\n",
      "Training:  Avg loss: 0.448229 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.479268 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.348248  [   64/60000]\n",
      "loss: 0.460315  [ 6464/60000]\n",
      "loss: 0.394220  [12864/60000]\n",
      "loss: 0.237507  [19264/60000]\n",
      "loss: 0.324189  [25664/60000]\n",
      "loss: 0.493509  [32064/60000]\n",
      "loss: 0.536278  [38464/60000]\n",
      "loss: 0.457667  [44864/60000]\n",
      "loss: 0.526438  [51264/60000]\n",
      "loss: 0.412583  [57664/60000]\n",
      "Training:  Avg loss: 0.446374 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.477319 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.425347  [   64/60000]\n",
      "loss: 0.551222  [ 6464/60000]\n",
      "loss: 0.378228  [12864/60000]\n",
      "loss: 0.416904  [19264/60000]\n",
      "loss: 0.517596  [25664/60000]\n",
      "loss: 0.344466  [32064/60000]\n",
      "loss: 0.542316  [38464/60000]\n",
      "loss: 0.462419  [44864/60000]\n",
      "loss: 0.516156  [51264/60000]\n",
      "loss: 0.566451  [57664/60000]\n",
      "Training:  Avg loss: 0.445015 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.475793 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.344783  [   64/60000]\n",
      "loss: 0.342858  [ 6464/60000]\n",
      "loss: 0.305156  [12864/60000]\n",
      "loss: 0.312549  [19264/60000]\n",
      "loss: 0.610292  [25664/60000]\n",
      "loss: 0.444416  [32064/60000]\n",
      "loss: 0.230068  [38464/60000]\n",
      "loss: 0.665709  [44864/60000]\n",
      "loss: 0.497227  [51264/60000]\n",
      "loss: 0.250939  [57664/60000]\n",
      "Training:  Avg loss: 0.443220 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.475537 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.482654  [   64/60000]\n",
      "loss: 0.624587  [ 6464/60000]\n",
      "loss: 0.576718  [12864/60000]\n",
      "loss: 0.460783  [19264/60000]\n",
      "loss: 0.427247  [25664/60000]\n",
      "loss: 0.435948  [32064/60000]\n",
      "loss: 0.621536  [38464/60000]\n",
      "loss: 0.280746  [44864/60000]\n",
      "loss: 0.482351  [51264/60000]\n",
      "loss: 0.559616  [57664/60000]\n",
      "Training:  Avg loss: 0.441760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.473551 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.473087  [   64/60000]\n",
      "loss: 0.494453  [ 6464/60000]\n",
      "loss: 0.302454  [12864/60000]\n",
      "loss: 0.356073  [19264/60000]\n",
      "loss: 0.434400  [25664/60000]\n",
      "loss: 0.264127  [32064/60000]\n",
      "loss: 0.573195  [38464/60000]\n",
      "loss: 0.444839  [44864/60000]\n",
      "loss: 0.447956  [51264/60000]\n",
      "loss: 0.406892  [57664/60000]\n",
      "Training:  Avg loss: 0.440428 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.473878 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.357538  [   64/60000]\n",
      "loss: 0.448943  [ 6464/60000]\n",
      "loss: 0.462811  [12864/60000]\n",
      "loss: 0.543150  [19264/60000]\n",
      "loss: 0.512387  [25664/60000]\n",
      "loss: 0.609214  [32064/60000]\n",
      "loss: 0.455824  [38464/60000]\n",
      "loss: 0.389456  [44864/60000]\n",
      "loss: 0.393633  [51264/60000]\n",
      "loss: 0.457481  [57664/60000]\n",
      "Training:  Avg loss: 0.438621 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.471484 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.449807  [   64/60000]\n",
      "loss: 0.522224  [ 6464/60000]\n",
      "loss: 0.256177  [12864/60000]\n",
      "loss: 0.698283  [19264/60000]\n",
      "loss: 0.331775  [25664/60000]\n",
      "loss: 0.366701  [32064/60000]\n",
      "loss: 0.508422  [38464/60000]\n",
      "loss: 0.428318  [44864/60000]\n",
      "loss: 0.420488  [51264/60000]\n",
      "loss: 0.352257  [57664/60000]\n",
      "Training:  Avg loss: 0.437381 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.471560 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.430410  [   64/60000]\n",
      "loss: 0.349937  [ 6464/60000]\n",
      "loss: 0.415143  [12864/60000]\n",
      "loss: 0.350210  [19264/60000]\n",
      "loss: 0.313187  [25664/60000]\n",
      "loss: 0.344804  [32064/60000]\n",
      "loss: 0.340716  [38464/60000]\n",
      "loss: 0.475910  [44864/60000]\n",
      "loss: 0.325805  [51264/60000]\n",
      "loss: 0.561410  [57664/60000]\n",
      "Training:  Avg loss: 0.436045 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.471648 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.317930  [   64/60000]\n",
      "loss: 0.374289  [ 6464/60000]\n",
      "loss: 0.498595  [12864/60000]\n",
      "loss: 0.442968  [19264/60000]\n",
      "loss: 0.493835  [25664/60000]\n",
      "loss: 0.389432  [32064/60000]\n",
      "loss: 0.323870  [38464/60000]\n",
      "loss: 0.337309  [44864/60000]\n",
      "loss: 0.401935  [51264/60000]\n",
      "loss: 0.465469  [57664/60000]\n",
      "Training:  Avg loss: 0.434849 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.467267 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.459365  [   64/60000]\n",
      "loss: 0.400963  [ 6464/60000]\n",
      "loss: 0.296567  [12864/60000]\n",
      "loss: 0.576788  [19264/60000]\n",
      "loss: 0.491174  [25664/60000]\n",
      "loss: 0.593758  [32064/60000]\n",
      "loss: 0.541296  [38464/60000]\n",
      "loss: 0.661963  [44864/60000]\n",
      "loss: 0.393889  [51264/60000]\n",
      "loss: 0.380665  [57664/60000]\n",
      "Training:  Avg loss: 0.433121 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.465898 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.430113  [   64/60000]\n",
      "loss: 0.279608  [ 6464/60000]\n",
      "loss: 0.583693  [12864/60000]\n",
      "loss: 0.299438  [19264/60000]\n",
      "loss: 0.403249  [25664/60000]\n",
      "loss: 0.450505  [32064/60000]\n",
      "loss: 0.527653  [38464/60000]\n",
      "loss: 0.462532  [44864/60000]\n",
      "loss: 0.390101  [51264/60000]\n",
      "loss: 0.258771  [57664/60000]\n",
      "Training:  Avg loss: 0.431964 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.467063 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.438068  [   64/60000]\n",
      "loss: 0.440594  [ 6464/60000]\n",
      "loss: 0.474392  [12864/60000]\n",
      "loss: 0.338216  [19264/60000]\n",
      "loss: 0.421965  [25664/60000]\n",
      "loss: 0.728631  [32064/60000]\n",
      "loss: 0.560969  [38464/60000]\n",
      "loss: 0.304966  [44864/60000]\n",
      "loss: 0.419775  [51264/60000]\n",
      "loss: 0.275494  [57664/60000]\n",
      "Training:  Avg loss: 0.430687 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.469272 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.665837  [   64/60000]\n",
      "loss: 0.457592  [ 6464/60000]\n",
      "loss: 0.506523  [12864/60000]\n",
      "loss: 0.578363  [19264/60000]\n",
      "loss: 0.235150  [25664/60000]\n",
      "loss: 0.455532  [32064/60000]\n",
      "loss: 0.522893  [38464/60000]\n",
      "loss: 0.277060  [44864/60000]\n",
      "loss: 0.494128  [51264/60000]\n",
      "loss: 0.347335  [57664/60000]\n",
      "Training:  Avg loss: 0.429397 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.464869 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.541734  [   64/60000]\n",
      "loss: 0.400990  [ 6464/60000]\n",
      "loss: 0.247027  [12864/60000]\n",
      "loss: 0.426049  [19264/60000]\n",
      "loss: 0.447154  [25664/60000]\n",
      "loss: 0.326502  [32064/60000]\n",
      "loss: 0.328690  [38464/60000]\n",
      "loss: 0.600010  [44864/60000]\n",
      "loss: 0.476226  [51264/60000]\n",
      "loss: 0.424725  [57664/60000]\n",
      "Training:  Avg loss: 0.428301 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.464954 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.285354  [   64/60000]\n",
      "loss: 0.382679  [ 6464/60000]\n",
      "loss: 0.394141  [12864/60000]\n",
      "loss: 0.317934  [19264/60000]\n",
      "loss: 0.406156  [25664/60000]\n",
      "loss: 0.503245  [32064/60000]\n",
      "loss: 0.676990  [38464/60000]\n",
      "loss: 0.224851  [44864/60000]\n",
      "loss: 0.414336  [51264/60000]\n",
      "loss: 0.427205  [57664/60000]\n",
      "Training:  Avg loss: 0.427318 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.463099 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.430682  [   64/60000]\n",
      "loss: 0.392633  [ 6464/60000]\n",
      "loss: 0.479354  [12864/60000]\n",
      "loss: 0.277831  [19264/60000]\n",
      "loss: 0.319459  [25664/60000]\n",
      "loss: 0.373512  [32064/60000]\n",
      "loss: 0.276361  [38464/60000]\n",
      "loss: 0.537008  [44864/60000]\n",
      "loss: 0.405001  [51264/60000]\n",
      "loss: 0.362817  [57664/60000]\n",
      "Training:  Avg loss: 0.425697 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.461494 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.265005  [   64/60000]\n",
      "loss: 0.379774  [ 6464/60000]\n",
      "loss: 0.381667  [12864/60000]\n",
      "loss: 0.572250  [19264/60000]\n",
      "loss: 0.644241  [25664/60000]\n",
      "loss: 0.445165  [32064/60000]\n",
      "loss: 0.455661  [38464/60000]\n",
      "loss: 0.535882  [44864/60000]\n",
      "loss: 0.513127  [51264/60000]\n",
      "loss: 0.502438  [57664/60000]\n",
      "Training:  Avg loss: 0.424616 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.463140 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.462453  [   64/60000]\n",
      "loss: 0.303225  [ 6464/60000]\n",
      "loss: 0.428878  [12864/60000]\n",
      "loss: 0.455746  [19264/60000]\n",
      "loss: 0.327740  [25664/60000]\n",
      "loss: 0.324729  [32064/60000]\n",
      "loss: 0.637555  [38464/60000]\n",
      "loss: 0.473414  [44864/60000]\n",
      "loss: 0.404146  [51264/60000]\n",
      "loss: 0.400764  [57664/60000]\n",
      "Training:  Avg loss: 0.423516 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.458444 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.462510  [   64/60000]\n",
      "loss: 0.326252  [ 6464/60000]\n",
      "loss: 0.295564  [12864/60000]\n",
      "loss: 0.395780  [19264/60000]\n",
      "loss: 0.458473  [25664/60000]\n",
      "loss: 0.413781  [32064/60000]\n",
      "loss: 0.292992  [38464/60000]\n",
      "loss: 0.502408  [44864/60000]\n",
      "loss: 0.392485  [51264/60000]\n",
      "loss: 0.409678  [57664/60000]\n",
      "Training:  Avg loss: 0.422220 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.459717 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.581047  [   64/60000]\n",
      "loss: 0.382199  [ 6464/60000]\n",
      "loss: 0.411687  [12864/60000]\n",
      "loss: 0.267001  [19264/60000]\n",
      "loss: 0.332361  [25664/60000]\n",
      "loss: 0.408039  [32064/60000]\n",
      "loss: 0.450759  [38464/60000]\n",
      "loss: 0.518820  [44864/60000]\n",
      "loss: 0.506540  [51264/60000]\n",
      "loss: 0.457262  [57664/60000]\n",
      "Training:  Avg loss: 0.421258 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.455261 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.780045  [   64/60000]\n",
      "loss: 0.525347  [ 6464/60000]\n",
      "loss: 0.445660  [12864/60000]\n",
      "loss: 0.399982  [19264/60000]\n",
      "loss: 0.461532  [25664/60000]\n",
      "loss: 0.539729  [32064/60000]\n",
      "loss: 0.482346  [38464/60000]\n",
      "loss: 0.266486  [44864/60000]\n",
      "loss: 0.369866  [51264/60000]\n",
      "loss: 0.313017  [57664/60000]\n",
      "Training:  Avg loss: 0.420203 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.457710 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.392037  [   64/60000]\n",
      "loss: 0.447277  [ 6464/60000]\n",
      "loss: 0.432146  [12864/60000]\n",
      "loss: 0.407984  [19264/60000]\n",
      "loss: 0.543687  [25664/60000]\n",
      "loss: 0.433685  [32064/60000]\n",
      "loss: 0.480195  [38464/60000]\n",
      "loss: 0.511988  [44864/60000]\n",
      "loss: 0.596456  [51264/60000]\n",
      "loss: 0.514595  [57664/60000]\n",
      "Training:  Avg loss: 0.419250 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.456926 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.406291  [   64/60000]\n",
      "loss: 0.544812  [ 6464/60000]\n",
      "loss: 0.551551  [12864/60000]\n",
      "loss: 0.402233  [19264/60000]\n",
      "loss: 0.368116  [25664/60000]\n",
      "loss: 0.449351  [32064/60000]\n",
      "loss: 0.308450  [38464/60000]\n",
      "loss: 0.429140  [44864/60000]\n",
      "loss: 0.435456  [51264/60000]\n",
      "loss: 0.393449  [57664/60000]\n",
      "Training:  Avg loss: 0.418057 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.455548 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.396296  [   64/60000]\n",
      "loss: 0.347204  [ 6464/60000]\n",
      "loss: 0.645760  [12864/60000]\n",
      "loss: 0.433068  [19264/60000]\n",
      "loss: 0.457943  [25664/60000]\n",
      "loss: 0.418560  [32064/60000]\n",
      "loss: 0.370496  [38464/60000]\n",
      "loss: 0.583825  [44864/60000]\n",
      "loss: 0.456872  [51264/60000]\n",
      "loss: 0.331096  [57664/60000]\n",
      "Training:  Avg loss: 0.417001 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.453279 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.316948  [   64/60000]\n",
      "loss: 0.306610  [ 6464/60000]\n",
      "loss: 0.297536  [12864/60000]\n",
      "loss: 0.388517  [19264/60000]\n",
      "loss: 0.381947  [25664/60000]\n",
      "loss: 0.496729  [32064/60000]\n",
      "loss: 0.324425  [38464/60000]\n",
      "loss: 0.428094  [44864/60000]\n",
      "loss: 0.524903  [51264/60000]\n",
      "loss: 0.750488  [57664/60000]\n",
      "Training:  Avg loss: 0.415917 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.451304 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.417142  [   64/60000]\n",
      "loss: 0.391778  [ 6464/60000]\n",
      "loss: 0.324431  [12864/60000]\n",
      "loss: 0.358166  [19264/60000]\n",
      "loss: 0.342382  [25664/60000]\n",
      "loss: 0.608092  [32064/60000]\n",
      "loss: 0.351921  [38464/60000]\n",
      "loss: 0.583400  [44864/60000]\n",
      "loss: 0.386902  [51264/60000]\n",
      "loss: 0.407987  [57664/60000]\n",
      "Training:  Avg loss: 0.414931 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.451468 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.415069  [   64/60000]\n",
      "loss: 0.337351  [ 6464/60000]\n",
      "loss: 0.432335  [12864/60000]\n",
      "loss: 0.461074  [19264/60000]\n",
      "loss: 0.413632  [25664/60000]\n",
      "loss: 0.357964  [32064/60000]\n",
      "loss: 0.639341  [38464/60000]\n",
      "loss: 0.297142  [44864/60000]\n",
      "loss: 0.633826  [51264/60000]\n",
      "loss: 0.292246  [57664/60000]\n",
      "Training:  Avg loss: 0.413938 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.450730 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.450232  [   64/60000]\n",
      "loss: 0.531917  [ 6464/60000]\n",
      "loss: 0.686731  [12864/60000]\n",
      "loss: 0.469999  [19264/60000]\n",
      "loss: 0.481054  [25664/60000]\n",
      "loss: 0.352185  [32064/60000]\n",
      "loss: 0.449672  [38464/60000]\n",
      "loss: 0.394840  [44864/60000]\n",
      "loss: 0.367249  [51264/60000]\n",
      "loss: 0.609227  [57664/60000]\n",
      "Training:  Avg loss: 0.413144 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.448534 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.585889  [   64/60000]\n",
      "loss: 0.266412  [ 6464/60000]\n",
      "loss: 0.298764  [12864/60000]\n",
      "loss: 0.509440  [19264/60000]\n",
      "loss: 0.324644  [25664/60000]\n",
      "loss: 0.433403  [32064/60000]\n",
      "loss: 0.363969  [38464/60000]\n",
      "loss: 0.455944  [44864/60000]\n",
      "loss: 0.366680  [51264/60000]\n",
      "loss: 0.379570  [57664/60000]\n",
      "Training:  Avg loss: 0.412099 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.450313 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.492580  [   64/60000]\n",
      "loss: 0.232175  [ 6464/60000]\n",
      "loss: 0.631897  [12864/60000]\n",
      "loss: 0.404001  [19264/60000]\n",
      "loss: 0.386522  [25664/60000]\n",
      "loss: 0.484503  [32064/60000]\n",
      "loss: 0.323064  [38464/60000]\n",
      "loss: 0.587526  [44864/60000]\n",
      "loss: 0.357238  [51264/60000]\n",
      "loss: 0.397638  [57664/60000]\n",
      "Training:  Avg loss: 0.411090 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.447436 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.433773  [   64/60000]\n",
      "loss: 0.356648  [ 6464/60000]\n",
      "loss: 0.348493  [12864/60000]\n",
      "loss: 0.432889  [19264/60000]\n",
      "loss: 0.641604  [25664/60000]\n",
      "loss: 0.384276  [32064/60000]\n",
      "loss: 0.430546  [38464/60000]\n",
      "loss: 0.528326  [44864/60000]\n",
      "loss: 0.412296  [51264/60000]\n",
      "loss: 0.388848  [57664/60000]\n",
      "Training:  Avg loss: 0.409688 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.451847 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.412452  [   64/60000]\n",
      "loss: 0.272021  [ 6464/60000]\n",
      "loss: 0.448991  [12864/60000]\n",
      "loss: 0.315236  [19264/60000]\n",
      "loss: 0.494144  [25664/60000]\n",
      "loss: 0.435756  [32064/60000]\n",
      "loss: 0.255763  [38464/60000]\n",
      "loss: 0.489391  [44864/60000]\n",
      "loss: 0.353516  [51264/60000]\n",
      "loss: 0.426436  [57664/60000]\n",
      "Training:  Avg loss: 0.409009 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.448023 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.391498  [   64/60000]\n",
      "loss: 0.288007  [ 6464/60000]\n",
      "loss: 0.394185  [12864/60000]\n",
      "loss: 0.364094  [19264/60000]\n",
      "loss: 0.427647  [25664/60000]\n",
      "loss: 0.236352  [32064/60000]\n",
      "loss: 0.519126  [38464/60000]\n",
      "loss: 0.355294  [44864/60000]\n",
      "loss: 0.397996  [51264/60000]\n",
      "loss: 0.391982  [57664/60000]\n",
      "Training:  Avg loss: 0.408171 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.447745 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.455590  [   64/60000]\n",
      "loss: 0.305056  [ 6464/60000]\n",
      "loss: 0.648889  [12864/60000]\n",
      "loss: 0.280392  [19264/60000]\n",
      "loss: 0.282052  [25664/60000]\n",
      "loss: 0.274996  [32064/60000]\n",
      "loss: 0.394519  [38464/60000]\n",
      "loss: 0.352493  [44864/60000]\n",
      "loss: 0.304131  [51264/60000]\n",
      "loss: 0.612631  [57664/60000]\n",
      "Training:  Avg loss: 0.407080 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.444868 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.410997  [   64/60000]\n",
      "loss: 0.292060  [ 6464/60000]\n",
      "loss: 0.418635  [12864/60000]\n",
      "loss: 0.448416  [19264/60000]\n",
      "loss: 0.484274  [25664/60000]\n",
      "loss: 0.403036  [32064/60000]\n",
      "loss: 0.290501  [38464/60000]\n",
      "loss: 0.620538  [44864/60000]\n",
      "loss: 0.451989  [51264/60000]\n",
      "loss: 0.379933  [57664/60000]\n",
      "Training:  Avg loss: 0.406168 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.445761 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.345847  [   64/60000]\n",
      "loss: 0.476054  [ 6464/60000]\n",
      "loss: 0.443032  [12864/60000]\n",
      "loss: 0.382314  [19264/60000]\n",
      "loss: 0.415116  [25664/60000]\n",
      "loss: 0.542080  [32064/60000]\n",
      "loss: 0.369742  [38464/60000]\n",
      "loss: 0.279306  [44864/60000]\n",
      "loss: 0.362536  [51264/60000]\n",
      "loss: 0.484418  [57664/60000]\n",
      "Training:  Avg loss: 0.405224 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.444071 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.430154  [   64/60000]\n",
      "loss: 0.321012  [ 6464/60000]\n",
      "loss: 0.339986  [12864/60000]\n",
      "loss: 0.476474  [19264/60000]\n",
      "loss: 0.308330  [25664/60000]\n",
      "loss: 0.361824  [32064/60000]\n",
      "loss: 0.339204  [38464/60000]\n",
      "loss: 0.459025  [44864/60000]\n",
      "loss: 0.374918  [51264/60000]\n",
      "loss: 0.440573  [57664/60000]\n",
      "Training:  Avg loss: 0.404375 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.442041 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.320040  [   64/60000]\n",
      "loss: 0.516108  [ 6464/60000]\n",
      "loss: 0.444450  [12864/60000]\n",
      "loss: 0.508035  [19264/60000]\n",
      "loss: 0.498286  [25664/60000]\n",
      "loss: 0.411737  [32064/60000]\n",
      "loss: 0.644308  [38464/60000]\n",
      "loss: 0.449005  [44864/60000]\n",
      "loss: 0.472792  [51264/60000]\n",
      "loss: 0.248680  [57664/60000]\n",
      "Training:  Avg loss: 0.403517 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.441518 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.301211  [   64/60000]\n",
      "loss: 0.415157  [ 6464/60000]\n",
      "loss: 0.425654  [12864/60000]\n",
      "loss: 0.391975  [19264/60000]\n",
      "loss: 0.332186  [25664/60000]\n",
      "loss: 0.469436  [32064/60000]\n",
      "loss: 0.373725  [38464/60000]\n",
      "loss: 0.327909  [44864/60000]\n",
      "loss: 0.421486  [51264/60000]\n",
      "loss: 0.278073  [57664/60000]\n",
      "Training:  Avg loss: 0.402606 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.443069 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.494374  [   64/60000]\n",
      "loss: 0.552920  [ 6464/60000]\n",
      "loss: 0.408851  [12864/60000]\n",
      "loss: 0.409173  [19264/60000]\n",
      "loss: 0.379750  [25664/60000]\n",
      "loss: 0.414319  [32064/60000]\n",
      "loss: 0.270047  [38464/60000]\n",
      "loss: 0.380667  [44864/60000]\n",
      "loss: 0.385194  [51264/60000]\n",
      "loss: 0.548069  [57664/60000]\n",
      "Training:  Avg loss: 0.401636 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.439778 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.486943  [   64/60000]\n",
      "loss: 0.341472  [ 6464/60000]\n",
      "loss: 0.401837  [12864/60000]\n",
      "loss: 0.546999  [19264/60000]\n",
      "loss: 0.478539  [25664/60000]\n",
      "loss: 0.475511  [32064/60000]\n",
      "loss: 0.300177  [38464/60000]\n",
      "loss: 0.404018  [44864/60000]\n",
      "loss: 0.588388  [51264/60000]\n",
      "loss: 0.454172  [57664/60000]\n",
      "Training:  Avg loss: 0.400654 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.439702 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.431119  [   64/60000]\n",
      "loss: 0.419424  [ 6464/60000]\n",
      "loss: 0.293731  [12864/60000]\n",
      "loss: 0.229879  [19264/60000]\n",
      "loss: 0.221745  [25664/60000]\n",
      "loss: 0.280367  [32064/60000]\n",
      "loss: 0.446282  [38464/60000]\n",
      "loss: 0.279819  [44864/60000]\n",
      "loss: 0.425663  [51264/60000]\n",
      "loss: 0.385917  [57664/60000]\n",
      "Training:  Avg loss: 0.399917 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.439605 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.426570  [   64/60000]\n",
      "loss: 0.468221  [ 6464/60000]\n",
      "loss: 0.632124  [12864/60000]\n",
      "loss: 0.402391  [19264/60000]\n",
      "loss: 0.400133  [25664/60000]\n",
      "loss: 0.253132  [32064/60000]\n",
      "loss: 0.151277  [38464/60000]\n",
      "loss: 0.506949  [44864/60000]\n",
      "loss: 0.411248  [51264/60000]\n",
      "loss: 0.400106  [57664/60000]\n",
      "Training:  Avg loss: 0.399057 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.438615 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.509882  [   64/60000]\n",
      "loss: 0.512010  [ 6464/60000]\n",
      "loss: 0.271532  [12864/60000]\n",
      "loss: 0.318499  [19264/60000]\n",
      "loss: 0.570976  [25664/60000]\n",
      "loss: 0.245911  [32064/60000]\n",
      "loss: 0.621879  [38464/60000]\n",
      "loss: 0.358898  [44864/60000]\n",
      "loss: 0.263091  [51264/60000]\n",
      "loss: 0.680865  [57664/60000]\n",
      "Training:  Avg loss: 0.398121 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.437417 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.451396  [   64/60000]\n",
      "loss: 0.391990  [ 6464/60000]\n",
      "loss: 0.605402  [12864/60000]\n",
      "loss: 0.335546  [19264/60000]\n",
      "loss: 0.418877  [25664/60000]\n",
      "loss: 0.403074  [32064/60000]\n",
      "loss: 0.281607  [38464/60000]\n",
      "loss: 0.573871  [44864/60000]\n",
      "loss: 0.468469  [51264/60000]\n",
      "loss: 0.361584  [57664/60000]\n",
      "Training:  Avg loss: 0.397410 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.435655 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.233587  [   64/60000]\n",
      "loss: 0.368799  [ 6464/60000]\n",
      "loss: 0.530083  [12864/60000]\n",
      "loss: 0.382134  [19264/60000]\n",
      "loss: 0.320046  [25664/60000]\n",
      "loss: 0.300588  [32064/60000]\n",
      "loss: 0.351332  [38464/60000]\n",
      "loss: 0.384706  [44864/60000]\n",
      "loss: 0.366852  [51264/60000]\n",
      "loss: 0.622273  [57664/60000]\n",
      "Training:  Avg loss: 0.396712 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.436481 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.437855  [   64/60000]\n",
      "loss: 0.542580  [ 6464/60000]\n",
      "loss: 0.327184  [12864/60000]\n",
      "loss: 0.509392  [19264/60000]\n",
      "loss: 0.293672  [25664/60000]\n",
      "loss: 0.415976  [32064/60000]\n",
      "loss: 0.435125  [38464/60000]\n",
      "loss: 0.423195  [44864/60000]\n",
      "loss: 0.580295  [51264/60000]\n",
      "loss: 0.392360  [57664/60000]\n",
      "Training:  Avg loss: 0.395874 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.436611 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.245615  [   64/60000]\n",
      "loss: 0.287169  [ 6464/60000]\n",
      "loss: 0.503762  [12864/60000]\n",
      "loss: 0.456056  [19264/60000]\n",
      "loss: 0.402944  [25664/60000]\n",
      "loss: 0.350891  [32064/60000]\n",
      "loss: 0.381690  [38464/60000]\n",
      "loss: 0.268994  [44864/60000]\n",
      "loss: 0.370637  [51264/60000]\n",
      "loss: 0.295888  [57664/60000]\n",
      "Training:  Avg loss: 0.394827 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.433958 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.471266  [   64/60000]\n",
      "loss: 0.275193  [ 6464/60000]\n",
      "loss: 0.371168  [12864/60000]\n",
      "loss: 0.435232  [19264/60000]\n",
      "loss: 0.479366  [25664/60000]\n",
      "loss: 0.284882  [32064/60000]\n",
      "loss: 0.276630  [38464/60000]\n",
      "loss: 0.253451  [44864/60000]\n",
      "loss: 0.299919  [51264/60000]\n",
      "loss: 0.560735  [57664/60000]\n",
      "Training:  Avg loss: 0.393936 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.434158 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.465356  [   64/60000]\n",
      "loss: 0.227235  [ 6464/60000]\n",
      "loss: 0.256349  [12864/60000]\n",
      "loss: 0.550946  [19264/60000]\n",
      "loss: 0.366209  [25664/60000]\n",
      "loss: 0.326855  [32064/60000]\n",
      "loss: 0.418458  [38464/60000]\n",
      "loss: 0.387467  [44864/60000]\n",
      "loss: 0.618898  [51264/60000]\n",
      "loss: 0.367515  [57664/60000]\n",
      "Training:  Avg loss: 0.393070 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.433917 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.376574  [   64/60000]\n",
      "loss: 0.402557  [ 6464/60000]\n",
      "loss: 0.400091  [12864/60000]\n",
      "loss: 0.220421  [19264/60000]\n",
      "loss: 0.447474  [25664/60000]\n",
      "loss: 0.373322  [32064/60000]\n",
      "loss: 0.434123  [38464/60000]\n",
      "loss: 0.388140  [44864/60000]\n",
      "loss: 0.362373  [51264/60000]\n",
      "loss: 0.333742  [57664/60000]\n",
      "Training:  Avg loss: 0.392351 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.431634 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.250650  [   64/60000]\n",
      "loss: 0.448031  [ 6464/60000]\n",
      "loss: 0.476818  [12864/60000]\n",
      "loss: 0.345981  [19264/60000]\n",
      "loss: 0.454222  [25664/60000]\n",
      "loss: 0.535613  [32064/60000]\n",
      "loss: 0.373346  [38464/60000]\n",
      "loss: 0.413688  [44864/60000]\n",
      "loss: 0.240358  [51264/60000]\n",
      "loss: 0.324924  [57664/60000]\n",
      "Training:  Avg loss: 0.391540 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.434816 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.536765  [   64/60000]\n",
      "loss: 0.295918  [ 6464/60000]\n",
      "loss: 0.252102  [12864/60000]\n",
      "loss: 0.299521  [19264/60000]\n",
      "loss: 0.396369  [25664/60000]\n",
      "loss: 0.308705  [32064/60000]\n",
      "loss: 0.369270  [38464/60000]\n",
      "loss: 0.493707  [44864/60000]\n",
      "loss: 0.407685  [51264/60000]\n",
      "loss: 0.363080  [57664/60000]\n",
      "Training:  Avg loss: 0.390975 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.432403 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.355015  [   64/60000]\n",
      "loss: 0.202189  [ 6464/60000]\n",
      "loss: 0.376995  [12864/60000]\n",
      "loss: 0.414441  [19264/60000]\n",
      "loss: 0.254836  [25664/60000]\n",
      "loss: 0.522339  [32064/60000]\n",
      "loss: 0.257781  [38464/60000]\n",
      "loss: 0.344686  [44864/60000]\n",
      "loss: 0.198559  [51264/60000]\n",
      "loss: 0.538318  [57664/60000]\n",
      "Training:  Avg loss: 0.389837 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.430723 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.433278  [   64/60000]\n",
      "loss: 0.521938  [ 6464/60000]\n",
      "loss: 0.280732  [12864/60000]\n",
      "loss: 0.332281  [19264/60000]\n",
      "loss: 0.334278  [25664/60000]\n",
      "loss: 0.464773  [32064/60000]\n",
      "loss: 0.249983  [38464/60000]\n",
      "loss: 0.386614  [44864/60000]\n",
      "loss: 0.213722  [51264/60000]\n",
      "loss: 0.388266  [57664/60000]\n",
      "Training:  Avg loss: 0.389157 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.431474 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.424223  [   64/60000]\n",
      "loss: 0.418272  [ 6464/60000]\n",
      "loss: 0.428484  [12864/60000]\n",
      "loss: 0.254948  [19264/60000]\n",
      "loss: 0.445914  [25664/60000]\n",
      "loss: 0.328918  [32064/60000]\n",
      "loss: 0.439638  [38464/60000]\n",
      "loss: 0.383240  [44864/60000]\n",
      "loss: 0.270202  [51264/60000]\n",
      "loss: 0.414970  [57664/60000]\n",
      "Training:  Avg loss: 0.388512 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.430035 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.461811  [   64/60000]\n",
      "loss: 0.471480  [ 6464/60000]\n",
      "loss: 0.476541  [12864/60000]\n",
      "loss: 0.243049  [19264/60000]\n",
      "loss: 0.417113  [25664/60000]\n",
      "loss: 0.321410  [32064/60000]\n",
      "loss: 0.406776  [38464/60000]\n",
      "loss: 0.338137  [44864/60000]\n",
      "loss: 0.638705  [51264/60000]\n",
      "loss: 0.561096  [57664/60000]\n",
      "Training:  Avg loss: 0.387673 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.428028 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.475718  [   64/60000]\n",
      "loss: 0.368880  [ 6464/60000]\n",
      "loss: 0.342176  [12864/60000]\n",
      "loss: 0.228068  [19264/60000]\n",
      "loss: 0.370470  [25664/60000]\n",
      "loss: 0.412414  [32064/60000]\n",
      "loss: 0.602007  [38464/60000]\n",
      "loss: 0.476430  [44864/60000]\n",
      "loss: 0.490187  [51264/60000]\n",
      "loss: 0.378609  [57664/60000]\n",
      "Training:  Avg loss: 0.386956 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.427691 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.459164  [   64/60000]\n",
      "loss: 0.535132  [ 6464/60000]\n",
      "loss: 0.353040  [12864/60000]\n",
      "loss: 0.502918  [19264/60000]\n",
      "loss: 0.453084  [25664/60000]\n",
      "loss: 0.508579  [32064/60000]\n",
      "loss: 0.308242  [38464/60000]\n",
      "loss: 0.435001  [44864/60000]\n",
      "loss: 0.233482  [51264/60000]\n",
      "loss: 0.277727  [57664/60000]\n",
      "Training:  Avg loss: 0.386110 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.431153 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.490627  [   64/60000]\n",
      "loss: 0.452295  [ 6464/60000]\n",
      "loss: 0.346943  [12864/60000]\n",
      "loss: 0.294683  [19264/60000]\n",
      "loss: 0.384553  [25664/60000]\n",
      "loss: 0.326303  [32064/60000]\n",
      "loss: 0.520458  [38464/60000]\n",
      "loss: 0.518151  [44864/60000]\n",
      "loss: 0.372662  [51264/60000]\n",
      "loss: 0.252861  [57664/60000]\n",
      "Training:  Avg loss: 0.385241 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.433423 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.373600  [   64/60000]\n",
      "loss: 0.331585  [ 6464/60000]\n",
      "loss: 0.169209  [12864/60000]\n",
      "loss: 0.231002  [19264/60000]\n",
      "loss: 0.398774  [25664/60000]\n",
      "loss: 0.530707  [32064/60000]\n",
      "loss: 0.389098  [38464/60000]\n",
      "loss: 0.321459  [44864/60000]\n",
      "loss: 0.511757  [51264/60000]\n",
      "loss: 0.276162  [57664/60000]\n",
      "Training:  Avg loss: 0.384671 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.428415 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.401863  [   64/60000]\n",
      "loss: 0.268430  [ 6464/60000]\n",
      "loss: 0.369191  [12864/60000]\n",
      "loss: 0.304229  [19264/60000]\n",
      "loss: 0.344461  [25664/60000]\n",
      "loss: 0.235739  [32064/60000]\n",
      "loss: 0.394610  [38464/60000]\n",
      "loss: 0.397096  [44864/60000]\n",
      "loss: 0.368147  [51264/60000]\n",
      "loss: 0.500880  [57664/60000]\n",
      "Training:  Avg loss: 0.383884 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.426114 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.285650  [   64/60000]\n",
      "loss: 0.192400  [ 6464/60000]\n",
      "loss: 0.347105  [12864/60000]\n",
      "loss: 0.330004  [19264/60000]\n",
      "loss: 0.527054  [25664/60000]\n",
      "loss: 0.440295  [32064/60000]\n",
      "loss: 0.294425  [38464/60000]\n",
      "loss: 0.260993  [44864/60000]\n",
      "loss: 0.225471  [51264/60000]\n",
      "loss: 0.373775  [57664/60000]\n",
      "Training:  Avg loss: 0.382940 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.426285 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.541698  [   64/60000]\n",
      "loss: 0.419134  [ 6464/60000]\n",
      "loss: 0.320236  [12864/60000]\n",
      "loss: 0.548963  [19264/60000]\n",
      "loss: 0.430534  [25664/60000]\n",
      "loss: 0.296816  [32064/60000]\n",
      "loss: 0.531143  [38464/60000]\n",
      "loss: 0.400743  [44864/60000]\n",
      "loss: 0.407503  [51264/60000]\n",
      "loss: 0.307511  [57664/60000]\n",
      "Training:  Avg loss: 0.382533 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.423507 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.425473  [   64/60000]\n",
      "loss: 0.261870  [ 6464/60000]\n",
      "loss: 0.385621  [12864/60000]\n",
      "loss: 0.342744  [19264/60000]\n",
      "loss: 0.301438  [25664/60000]\n",
      "loss: 0.441288  [32064/60000]\n",
      "loss: 0.360050  [38464/60000]\n",
      "loss: 0.507741  [44864/60000]\n",
      "loss: 0.254587  [51264/60000]\n",
      "loss: 0.718381  [57664/60000]\n",
      "Training:  Avg loss: 0.381644 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.427551 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.430225  [   64/60000]\n",
      "loss: 0.293911  [ 6464/60000]\n",
      "loss: 0.597617  [12864/60000]\n",
      "loss: 0.300005  [19264/60000]\n",
      "loss: 0.303512  [25664/60000]\n",
      "loss: 0.416875  [32064/60000]\n",
      "loss: 0.429760  [38464/60000]\n",
      "loss: 0.374106  [44864/60000]\n",
      "loss: 0.592351  [51264/60000]\n",
      "loss: 0.396400  [57664/60000]\n",
      "Training:  Avg loss: 0.380654 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.422868 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.540780  [   64/60000]\n",
      "loss: 0.245952  [ 6464/60000]\n",
      "loss: 0.454271  [12864/60000]\n",
      "loss: 0.372596  [19264/60000]\n",
      "loss: 0.497271  [25664/60000]\n",
      "loss: 0.474641  [32064/60000]\n",
      "loss: 0.454815  [38464/60000]\n",
      "loss: 0.287602  [44864/60000]\n",
      "loss: 0.515634  [51264/60000]\n",
      "loss: 0.377036  [57664/60000]\n",
      "Training:  Avg loss: 0.380167 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.423092 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.405707  [   64/60000]\n",
      "loss: 0.472511  [ 6464/60000]\n",
      "loss: 0.646109  [12864/60000]\n",
      "loss: 0.331364  [19264/60000]\n",
      "loss: 0.412125  [25664/60000]\n",
      "loss: 0.364820  [32064/60000]\n",
      "loss: 0.478512  [38464/60000]\n",
      "loss: 0.381571  [44864/60000]\n",
      "loss: 0.413404  [51264/60000]\n",
      "loss: 0.276485  [57664/60000]\n",
      "Training:  Avg loss: 0.379553 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.421595 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.263278  [   64/60000]\n",
      "loss: 0.246950  [ 6464/60000]\n",
      "loss: 0.244612  [12864/60000]\n",
      "loss: 0.307871  [19264/60000]\n",
      "loss: 0.425273  [25664/60000]\n",
      "loss: 0.452259  [32064/60000]\n",
      "loss: 0.214762  [38464/60000]\n",
      "loss: 0.361900  [44864/60000]\n",
      "loss: 0.340738  [51264/60000]\n",
      "loss: 0.377260  [57664/60000]\n",
      "Training:  Avg loss: 0.378861 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.421270 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.613696  [   64/60000]\n",
      "loss: 0.394075  [ 6464/60000]\n",
      "loss: 0.365353  [12864/60000]\n",
      "loss: 0.405280  [19264/60000]\n",
      "loss: 0.371695  [25664/60000]\n",
      "loss: 0.466122  [32064/60000]\n",
      "loss: 0.294914  [38464/60000]\n",
      "loss: 0.363054  [44864/60000]\n",
      "loss: 0.504049  [51264/60000]\n",
      "loss: 0.301643  [57664/60000]\n",
      "Training:  Avg loss: 0.378052 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.420127 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.518101  [   64/60000]\n",
      "loss: 0.288998  [ 6464/60000]\n",
      "loss: 0.425225  [12864/60000]\n",
      "loss: 0.306571  [19264/60000]\n",
      "loss: 0.302897  [25664/60000]\n",
      "loss: 0.262713  [32064/60000]\n",
      "loss: 0.483747  [38464/60000]\n",
      "loss: 0.337841  [44864/60000]\n",
      "loss: 0.285710  [51264/60000]\n",
      "loss: 0.288899  [57664/60000]\n",
      "Training:  Avg loss: 0.377271 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.422686 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.238905  [   64/60000]\n",
      "loss: 0.255904  [ 6464/60000]\n",
      "loss: 0.524132  [12864/60000]\n",
      "loss: 0.455754  [19264/60000]\n",
      "loss: 0.254928  [25664/60000]\n",
      "loss: 0.397629  [32064/60000]\n",
      "loss: 0.334830  [38464/60000]\n",
      "loss: 0.434878  [44864/60000]\n",
      "loss: 0.363428  [51264/60000]\n",
      "loss: 0.374416  [57664/60000]\n",
      "Training:  Avg loss: 0.376537 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.419240 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.361666  [   64/60000]\n",
      "loss: 0.456256  [ 6464/60000]\n",
      "loss: 0.348197  [12864/60000]\n",
      "loss: 0.311173  [19264/60000]\n",
      "loss: 0.326375  [25664/60000]\n",
      "loss: 0.429687  [32064/60000]\n",
      "loss: 0.316724  [38464/60000]\n",
      "loss: 0.426301  [44864/60000]\n",
      "loss: 0.462892  [51264/60000]\n",
      "loss: 0.364722  [57664/60000]\n",
      "Training:  Avg loss: 0.375696 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.418280 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.329003  [   64/60000]\n",
      "loss: 0.347957  [ 6464/60000]\n",
      "loss: 0.419920  [12864/60000]\n",
      "loss: 0.301389  [19264/60000]\n",
      "loss: 0.307867  [25664/60000]\n",
      "loss: 0.480428  [32064/60000]\n",
      "loss: 0.356830  [38464/60000]\n",
      "loss: 0.299625  [44864/60000]\n",
      "loss: 0.376961  [51264/60000]\n",
      "loss: 0.412850  [57664/60000]\n",
      "Training:  Avg loss: 0.375205 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.418991 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.274506  [   64/60000]\n",
      "loss: 0.459000  [ 6464/60000]\n",
      "loss: 0.421505  [12864/60000]\n",
      "loss: 0.349347  [19264/60000]\n",
      "loss: 0.436784  [25664/60000]\n",
      "loss: 0.481915  [32064/60000]\n",
      "loss: 0.373245  [38464/60000]\n",
      "loss: 0.416401  [44864/60000]\n",
      "loss: 0.494574  [51264/60000]\n",
      "loss: 0.318715  [57664/60000]\n",
      "Training:  Avg loss: 0.374474 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.418600 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.318407  [   64/60000]\n",
      "loss: 0.258378  [ 6464/60000]\n",
      "loss: 0.207923  [12864/60000]\n",
      "loss: 0.394039  [19264/60000]\n",
      "loss: 0.457506  [25664/60000]\n",
      "loss: 0.481952  [32064/60000]\n",
      "loss: 0.361362  [38464/60000]\n",
      "loss: 0.292621  [44864/60000]\n",
      "loss: 0.256606  [51264/60000]\n",
      "loss: 0.383615  [57664/60000]\n",
      "Training:  Avg loss: 0.373836 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.415623 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.441074  [   64/60000]\n",
      "loss: 0.427257  [ 6464/60000]\n",
      "loss: 0.434387  [12864/60000]\n",
      "loss: 0.324774  [19264/60000]\n",
      "loss: 0.257924  [25664/60000]\n",
      "loss: 0.392786  [32064/60000]\n",
      "loss: 0.449795  [38464/60000]\n",
      "loss: 0.515905  [44864/60000]\n",
      "loss: 0.605851  [51264/60000]\n",
      "loss: 0.291271  [57664/60000]\n",
      "Training:  Avg loss: 0.373189 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.419306 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.414908  [   64/60000]\n",
      "loss: 0.274982  [ 6464/60000]\n",
      "loss: 0.456474  [12864/60000]\n",
      "loss: 0.318982  [19264/60000]\n",
      "loss: 0.422933  [25664/60000]\n",
      "loss: 0.246733  [32064/60000]\n",
      "loss: 0.447563  [38464/60000]\n",
      "loss: 0.270850  [44864/60000]\n",
      "loss: 0.421835  [51264/60000]\n",
      "loss: 0.367731  [57664/60000]\n",
      "Training:  Avg loss: 0.372492 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.416542 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.300650  [   64/60000]\n",
      "loss: 0.665918  [ 6464/60000]\n",
      "loss: 0.342235  [12864/60000]\n",
      "loss: 0.391554  [19264/60000]\n",
      "loss: 0.439076  [25664/60000]\n",
      "loss: 0.439608  [32064/60000]\n",
      "loss: 0.383715  [38464/60000]\n",
      "loss: 0.224761  [44864/60000]\n",
      "loss: 0.437863  [51264/60000]\n",
      "loss: 0.572025  [57664/60000]\n",
      "Training:  Avg loss: 0.371638 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.419865 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.419008  [   64/60000]\n",
      "loss: 0.320820  [ 6464/60000]\n",
      "loss: 0.442377  [12864/60000]\n",
      "loss: 0.438523  [19264/60000]\n",
      "loss: 0.414585  [25664/60000]\n",
      "loss: 0.367565  [32064/60000]\n",
      "loss: 0.431798  [38464/60000]\n",
      "loss: 0.756817  [44864/60000]\n",
      "loss: 0.365452  [51264/60000]\n",
      "loss: 0.345561  [57664/60000]\n",
      "Training:  Avg loss: 0.371059 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.416294 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.398943  [   64/60000]\n",
      "loss: 0.242583  [ 6464/60000]\n",
      "loss: 0.349876  [12864/60000]\n",
      "loss: 0.575669  [19264/60000]\n",
      "loss: 0.253578  [25664/60000]\n",
      "loss: 0.228647  [32064/60000]\n",
      "loss: 0.410252  [38464/60000]\n",
      "loss: 0.611755  [44864/60000]\n",
      "loss: 0.460129  [51264/60000]\n",
      "loss: 0.396431  [57664/60000]\n",
      "Training:  Avg loss: 0.370469 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.415290 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.349043  [   64/60000]\n",
      "loss: 0.236890  [ 6464/60000]\n",
      "loss: 0.360623  [12864/60000]\n",
      "loss: 0.407081  [19264/60000]\n",
      "loss: 0.332843  [25664/60000]\n",
      "loss: 0.354736  [32064/60000]\n",
      "loss: 0.745227  [38464/60000]\n",
      "loss: 0.316520  [44864/60000]\n",
      "loss: 0.333470  [51264/60000]\n",
      "loss: 0.177794  [57664/60000]\n",
      "Training:  Avg loss: 0.369635 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.413827 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.332881  [   64/60000]\n",
      "loss: 0.404944  [ 6464/60000]\n",
      "loss: 0.566648  [12864/60000]\n",
      "loss: 0.297971  [19264/60000]\n",
      "loss: 0.307771  [25664/60000]\n",
      "loss: 0.333649  [32064/60000]\n",
      "loss: 0.263876  [38464/60000]\n",
      "loss: 0.324613  [44864/60000]\n",
      "loss: 0.478823  [51264/60000]\n",
      "loss: 0.459477  [57664/60000]\n",
      "Training:  Avg loss: 0.369073 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.413383 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.298304  [   64/60000]\n",
      "loss: 0.474981  [ 6464/60000]\n",
      "loss: 0.370209  [12864/60000]\n",
      "loss: 0.284372  [19264/60000]\n",
      "loss: 0.413460  [25664/60000]\n",
      "loss: 0.186880  [32064/60000]\n",
      "loss: 0.364167  [38464/60000]\n",
      "loss: 0.298813  [44864/60000]\n",
      "loss: 0.374118  [51264/60000]\n",
      "loss: 0.447629  [57664/60000]\n",
      "Training:  Avg loss: 0.368186 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.413980 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.364175  [   64/60000]\n",
      "loss: 0.329380  [ 6464/60000]\n",
      "loss: 0.249084  [12864/60000]\n",
      "loss: 0.306086  [19264/60000]\n",
      "loss: 0.432526  [25664/60000]\n",
      "loss: 0.473140  [32064/60000]\n",
      "loss: 0.154880  [38464/60000]\n",
      "loss: 0.273563  [44864/60000]\n",
      "loss: 0.567252  [51264/60000]\n",
      "loss: 0.470886  [57664/60000]\n",
      "Training:  Avg loss: 0.367664 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.413334 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.510455  [   64/60000]\n",
      "loss: 0.350772  [ 6464/60000]\n",
      "loss: 0.286215  [12864/60000]\n",
      "loss: 0.258948  [19264/60000]\n",
      "loss: 0.316249  [25664/60000]\n",
      "loss: 0.285508  [32064/60000]\n",
      "loss: 0.482896  [38464/60000]\n",
      "loss: 0.336938  [44864/60000]\n",
      "loss: 0.373401  [51264/60000]\n",
      "loss: 0.500124  [57664/60000]\n",
      "Training:  Avg loss: 0.367062 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.411559 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.171841  [   64/60000]\n",
      "loss: 0.343617  [ 6464/60000]\n",
      "loss: 0.381938  [12864/60000]\n",
      "loss: 0.406198  [19264/60000]\n",
      "loss: 0.537101  [25664/60000]\n",
      "loss: 0.430293  [32064/60000]\n",
      "loss: 0.193316  [38464/60000]\n",
      "loss: 0.274840  [44864/60000]\n",
      "loss: 0.499048  [51264/60000]\n",
      "loss: 0.390735  [57664/60000]\n",
      "Training:  Avg loss: 0.366323 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.412475 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.342560  [   64/60000]\n",
      "loss: 0.310210  [ 6464/60000]\n",
      "loss: 0.462394  [12864/60000]\n",
      "loss: 0.247537  [19264/60000]\n",
      "loss: 0.338805  [25664/60000]\n",
      "loss: 0.334555  [32064/60000]\n",
      "loss: 0.453828  [38464/60000]\n",
      "loss: 0.473601  [44864/60000]\n",
      "loss: 0.336694  [51264/60000]\n",
      "loss: 0.532746  [57664/60000]\n",
      "Training:  Avg loss: 0.365713 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.409858 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.489959  [   64/60000]\n",
      "loss: 0.275247  [ 6464/60000]\n",
      "loss: 0.303638  [12864/60000]\n",
      "loss: 0.268238  [19264/60000]\n",
      "loss: 0.353517  [25664/60000]\n",
      "loss: 0.433609  [32064/60000]\n",
      "loss: 0.315998  [38464/60000]\n",
      "loss: 0.263854  [44864/60000]\n",
      "loss: 0.258816  [51264/60000]\n",
      "loss: 0.485095  [57664/60000]\n",
      "Training:  Avg loss: 0.364978 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.408884 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.432611  [   64/60000]\n",
      "loss: 0.298973  [ 6464/60000]\n",
      "loss: 0.242857  [12864/60000]\n",
      "loss: 0.273537  [19264/60000]\n",
      "loss: 0.467589  [25664/60000]\n",
      "loss: 0.400536  [32064/60000]\n",
      "loss: 0.327411  [38464/60000]\n",
      "loss: 0.583998  [44864/60000]\n",
      "loss: 0.297130  [51264/60000]\n",
      "loss: 0.551187  [57664/60000]\n",
      "Training:  Avg loss: 0.364609 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.408716 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.379016  [   64/60000]\n",
      "loss: 0.343538  [ 6464/60000]\n",
      "loss: 0.546588  [12864/60000]\n",
      "loss: 0.344945  [19264/60000]\n",
      "loss: 0.426286  [25664/60000]\n",
      "loss: 0.342602  [32064/60000]\n",
      "loss: 0.506981  [38464/60000]\n",
      "loss: 0.583106  [44864/60000]\n",
      "loss: 0.360523  [51264/60000]\n",
      "loss: 0.258199  [57664/60000]\n",
      "Training:  Avg loss: 0.363610 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.408905 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.322325  [   64/60000]\n",
      "loss: 0.498031  [ 6464/60000]\n",
      "loss: 0.395905  [12864/60000]\n",
      "loss: 0.567068  [19264/60000]\n",
      "loss: 0.373881  [25664/60000]\n",
      "loss: 0.467297  [32064/60000]\n",
      "loss: 0.414890  [38464/60000]\n",
      "loss: 0.365311  [44864/60000]\n",
      "loss: 0.390132  [51264/60000]\n",
      "loss: 0.343145  [57664/60000]\n",
      "Training:  Avg loss: 0.363088 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.409075 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.647503  [   64/60000]\n",
      "loss: 0.567950  [ 6464/60000]\n",
      "loss: 0.376849  [12864/60000]\n",
      "loss: 0.424388  [19264/60000]\n",
      "loss: 0.435496  [25664/60000]\n",
      "loss: 0.469452  [32064/60000]\n",
      "loss: 0.340909  [38464/60000]\n",
      "loss: 0.290657  [44864/60000]\n",
      "loss: 0.415449  [51264/60000]\n",
      "loss: 0.348425  [57664/60000]\n",
      "Training:  Avg loss: 0.362393 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.407687 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.383797  [   64/60000]\n",
      "loss: 0.407611  [ 6464/60000]\n",
      "loss: 0.425906  [12864/60000]\n",
      "loss: 0.455536  [19264/60000]\n",
      "loss: 0.303180  [25664/60000]\n",
      "loss: 0.386315  [32064/60000]\n",
      "loss: 0.415596  [38464/60000]\n",
      "loss: 0.872373  [44864/60000]\n",
      "loss: 0.270081  [51264/60000]\n",
      "loss: 0.365254  [57664/60000]\n",
      "Training:  Avg loss: 0.361943 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.407433 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.289595  [   64/60000]\n",
      "loss: 0.251592  [ 6464/60000]\n",
      "loss: 0.401628  [12864/60000]\n",
      "loss: 0.165902  [19264/60000]\n",
      "loss: 0.381640  [25664/60000]\n",
      "loss: 0.589477  [32064/60000]\n",
      "loss: 0.390599  [38464/60000]\n",
      "loss: 0.431844  [44864/60000]\n",
      "loss: 0.411688  [51264/60000]\n",
      "loss: 0.213503  [57664/60000]\n",
      "Training:  Avg loss: 0.361003 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.408141 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.459467  [   64/60000]\n",
      "loss: 0.360704  [ 6464/60000]\n",
      "loss: 0.372739  [12864/60000]\n",
      "loss: 0.471468  [19264/60000]\n",
      "loss: 0.381510  [25664/60000]\n",
      "loss: 0.490247  [32064/60000]\n",
      "loss: 0.274536  [38464/60000]\n",
      "loss: 0.235974  [44864/60000]\n",
      "loss: 0.535991  [51264/60000]\n",
      "loss: 0.300826  [57664/60000]\n",
      "Training:  Avg loss: 0.360571 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.407128 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.403106  [   64/60000]\n",
      "loss: 0.257796  [ 6464/60000]\n",
      "loss: 0.469700  [12864/60000]\n",
      "loss: 0.312387  [19264/60000]\n",
      "loss: 0.299947  [25664/60000]\n",
      "loss: 0.211036  [32064/60000]\n",
      "loss: 0.244630  [38464/60000]\n",
      "loss: 0.276362  [44864/60000]\n",
      "loss: 0.332488  [51264/60000]\n",
      "loss: 0.347060  [57664/60000]\n",
      "Training:  Avg loss: 0.359961 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.406078 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.358342  [   64/60000]\n",
      "loss: 0.300331  [ 6464/60000]\n",
      "loss: 0.403381  [12864/60000]\n",
      "loss: 0.413488  [19264/60000]\n",
      "loss: 0.224365  [25664/60000]\n",
      "loss: 0.417212  [32064/60000]\n",
      "loss: 0.349578  [38464/60000]\n",
      "loss: 0.190166  [44864/60000]\n",
      "loss: 0.484806  [51264/60000]\n",
      "loss: 0.468278  [57664/60000]\n",
      "Training:  Avg loss: 0.359597 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.406293 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.236642  [   64/60000]\n",
      "loss: 0.256123  [ 6464/60000]\n",
      "loss: 0.417673  [12864/60000]\n",
      "loss: 0.393603  [19264/60000]\n",
      "loss: 0.494481  [25664/60000]\n",
      "loss: 0.320437  [32064/60000]\n",
      "loss: 0.306099  [38464/60000]\n",
      "loss: 0.398097  [44864/60000]\n",
      "loss: 0.291256  [51264/60000]\n",
      "loss: 0.368365  [57664/60000]\n",
      "Training:  Avg loss: 0.358648 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.407065 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.294543  [   64/60000]\n",
      "loss: 0.402395  [ 6464/60000]\n",
      "loss: 0.356626  [12864/60000]\n",
      "loss: 0.403457  [19264/60000]\n",
      "loss: 0.425125  [25664/60000]\n",
      "loss: 0.391875  [32064/60000]\n",
      "loss: 0.350374  [38464/60000]\n",
      "loss: 0.365869  [44864/60000]\n",
      "loss: 0.287339  [51264/60000]\n",
      "loss: 0.312117  [57664/60000]\n",
      "Training:  Avg loss: 0.358156 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.404423 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.276987  [   64/60000]\n",
      "loss: 0.320225  [ 6464/60000]\n",
      "loss: 0.341866  [12864/60000]\n",
      "loss: 0.329980  [19264/60000]\n",
      "loss: 0.285664  [25664/60000]\n",
      "loss: 0.336373  [32064/60000]\n",
      "loss: 0.469783  [38464/60000]\n",
      "loss: 0.558732  [44864/60000]\n",
      "loss: 0.417716  [51264/60000]\n",
      "loss: 0.328203  [57664/60000]\n",
      "Training:  Avg loss: 0.357661 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.406917 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.387687  [   64/60000]\n",
      "loss: 0.423965  [ 6464/60000]\n",
      "loss: 0.405118  [12864/60000]\n",
      "loss: 0.267872  [19264/60000]\n",
      "loss: 0.420843  [25664/60000]\n",
      "loss: 0.288485  [32064/60000]\n",
      "loss: 0.209838  [38464/60000]\n",
      "loss: 0.422156  [44864/60000]\n",
      "loss: 0.435960  [51264/60000]\n",
      "loss: 0.320071  [57664/60000]\n",
      "Training:  Avg loss: 0.356928 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.403155 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.313476  [   64/60000]\n",
      "loss: 0.537270  [ 6464/60000]\n",
      "loss: 0.116201  [12864/60000]\n",
      "loss: 0.532755  [19264/60000]\n",
      "loss: 0.340075  [25664/60000]\n",
      "loss: 0.330515  [32064/60000]\n",
      "loss: 0.257139  [38464/60000]\n",
      "loss: 0.210717  [44864/60000]\n",
      "loss: 0.385780  [51264/60000]\n",
      "loss: 0.302949  [57664/60000]\n",
      "Training:  Avg loss: 0.356106 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.405655 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.352467  [   64/60000]\n",
      "loss: 0.377474  [ 6464/60000]\n",
      "loss: 0.387985  [12864/60000]\n",
      "loss: 0.400335  [19264/60000]\n",
      "loss: 0.438629  [25664/60000]\n",
      "loss: 0.584053  [32064/60000]\n",
      "loss: 0.254772  [38464/60000]\n",
      "loss: 0.381575  [44864/60000]\n",
      "loss: 0.471902  [51264/60000]\n",
      "loss: 0.363390  [57664/60000]\n",
      "Training:  Avg loss: 0.355878 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.401817 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.556078  [   64/60000]\n",
      "loss: 0.331697  [ 6464/60000]\n",
      "loss: 0.366895  [12864/60000]\n",
      "loss: 0.437165  [19264/60000]\n",
      "loss: 0.397258  [25664/60000]\n",
      "loss: 0.363353  [32064/60000]\n",
      "loss: 0.408700  [38464/60000]\n",
      "loss: 0.200989  [44864/60000]\n",
      "loss: 0.258392  [51264/60000]\n",
      "loss: 0.471686  [57664/60000]\n",
      "Training:  Avg loss: 0.355145 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.402577 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.278808  [   64/60000]\n",
      "loss: 0.233664  [ 6464/60000]\n",
      "loss: 0.398664  [12864/60000]\n",
      "loss: 0.444521  [19264/60000]\n",
      "loss: 0.295744  [25664/60000]\n",
      "loss: 0.524682  [32064/60000]\n",
      "loss: 0.564111  [38464/60000]\n",
      "loss: 0.296059  [44864/60000]\n",
      "loss: 0.189642  [51264/60000]\n",
      "loss: 0.243697  [57664/60000]\n",
      "Training:  Avg loss: 0.354615 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.403378 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.240348  [   64/60000]\n",
      "loss: 0.225690  [ 6464/60000]\n",
      "loss: 0.342126  [12864/60000]\n",
      "loss: 0.372512  [19264/60000]\n",
      "loss: 0.308228  [25664/60000]\n",
      "loss: 0.374002  [32064/60000]\n",
      "loss: 0.304956  [38464/60000]\n",
      "loss: 0.377905  [44864/60000]\n",
      "loss: 0.304256  [51264/60000]\n",
      "loss: 0.138421  [57664/60000]\n",
      "Training:  Avg loss: 0.354046 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.402152 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.315744  [   64/60000]\n",
      "loss: 0.414380  [ 6464/60000]\n",
      "loss: 0.423431  [12864/60000]\n",
      "loss: 0.360429  [19264/60000]\n",
      "loss: 0.450496  [25664/60000]\n",
      "loss: 0.334781  [32064/60000]\n",
      "loss: 0.272894  [38464/60000]\n",
      "loss: 0.389230  [44864/60000]\n",
      "loss: 0.419703  [51264/60000]\n",
      "loss: 0.413864  [57664/60000]\n",
      "Training:  Avg loss: 0.353421 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.400842 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.414815  [   64/60000]\n",
      "loss: 0.329785  [ 6464/60000]\n",
      "loss: 0.566668  [12864/60000]\n",
      "loss: 0.237273  [19264/60000]\n",
      "loss: 0.469865  [25664/60000]\n",
      "loss: 0.287500  [32064/60000]\n",
      "loss: 0.320707  [38464/60000]\n",
      "loss: 0.487341  [44864/60000]\n",
      "loss: 0.417823  [51264/60000]\n",
      "loss: 0.448095  [57664/60000]\n",
      "Training:  Avg loss: 0.352996 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.401779 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.288094  [   64/60000]\n",
      "loss: 0.391540  [ 6464/60000]\n",
      "loss: 0.240253  [12864/60000]\n",
      "loss: 0.338359  [19264/60000]\n",
      "loss: 0.438738  [25664/60000]\n",
      "loss: 0.313344  [32064/60000]\n",
      "loss: 0.297727  [38464/60000]\n",
      "loss: 0.473933  [44864/60000]\n",
      "loss: 0.243998  [51264/60000]\n",
      "loss: 0.283680  [57664/60000]\n",
      "Training:  Avg loss: 0.352315 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.399473 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.261035  [   64/60000]\n",
      "loss: 0.232491  [ 6464/60000]\n",
      "loss: 0.187385  [12864/60000]\n",
      "loss: 0.370445  [19264/60000]\n",
      "loss: 0.421007  [25664/60000]\n",
      "loss: 0.250746  [32064/60000]\n",
      "loss: 0.298309  [38464/60000]\n",
      "loss: 0.401083  [44864/60000]\n",
      "loss: 0.342024  [51264/60000]\n",
      "loss: 0.408828  [57664/60000]\n",
      "Training:  Avg loss: 0.351790 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.398726 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.323233  [   64/60000]\n",
      "loss: 0.513868  [ 6464/60000]\n",
      "loss: 0.201259  [12864/60000]\n",
      "loss: 0.509965  [19264/60000]\n",
      "loss: 0.253939  [25664/60000]\n",
      "loss: 0.475852  [32064/60000]\n",
      "loss: 0.360505  [38464/60000]\n",
      "loss: 0.457542  [44864/60000]\n",
      "loss: 0.230107  [51264/60000]\n",
      "loss: 0.364698  [57664/60000]\n",
      "Training:  Avg loss: 0.351214 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.405439 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.454577  [   64/60000]\n",
      "loss: 0.275849  [ 6464/60000]\n",
      "loss: 0.321275  [12864/60000]\n",
      "loss: 0.370516  [19264/60000]\n",
      "loss: 0.445089  [25664/60000]\n",
      "loss: 0.292963  [32064/60000]\n",
      "loss: 0.377378  [38464/60000]\n",
      "loss: 0.261140  [44864/60000]\n",
      "loss: 0.502482  [51264/60000]\n",
      "loss: 0.251018  [57664/60000]\n",
      "Training:  Avg loss: 0.350388 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.398324 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.376497  [   64/60000]\n",
      "loss: 0.331113  [ 6464/60000]\n",
      "loss: 0.408217  [12864/60000]\n",
      "loss: 0.276195  [19264/60000]\n",
      "loss: 0.321285  [25664/60000]\n",
      "loss: 0.448846  [32064/60000]\n",
      "loss: 0.332747  [38464/60000]\n",
      "loss: 0.415327  [44864/60000]\n",
      "loss: 0.382042  [51264/60000]\n",
      "loss: 0.178891  [57664/60000]\n",
      "Training:  Avg loss: 0.349908 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.399231 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.385255  [   64/60000]\n",
      "loss: 0.278188  [ 6464/60000]\n",
      "loss: 0.232193  [12864/60000]\n",
      "loss: 0.342304  [19264/60000]\n",
      "loss: 0.380774  [25664/60000]\n",
      "loss: 0.443417  [32064/60000]\n",
      "loss: 0.264003  [38464/60000]\n",
      "loss: 0.286154  [44864/60000]\n",
      "loss: 0.236005  [51264/60000]\n",
      "loss: 0.437042  [57664/60000]\n",
      "Training:  Avg loss: 0.349278 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.401654 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.185688  [   64/60000]\n",
      "loss: 0.382931  [ 6464/60000]\n",
      "loss: 0.275224  [12864/60000]\n",
      "loss: 0.253659  [19264/60000]\n",
      "loss: 0.294743  [25664/60000]\n",
      "loss: 0.379249  [32064/60000]\n",
      "loss: 0.369161  [38464/60000]\n",
      "loss: 0.199774  [44864/60000]\n",
      "loss: 0.369162  [51264/60000]\n",
      "loss: 0.288104  [57664/60000]\n",
      "Training:  Avg loss: 0.349175 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.397584 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.395101  [   64/60000]\n",
      "loss: 0.376660  [ 6464/60000]\n",
      "loss: 0.345026  [12864/60000]\n",
      "loss: 0.450507  [19264/60000]\n",
      "loss: 0.248734  [25664/60000]\n",
      "loss: 0.332210  [32064/60000]\n",
      "loss: 0.341716  [38464/60000]\n",
      "loss: 0.226419  [44864/60000]\n",
      "loss: 0.389246  [51264/60000]\n",
      "loss: 0.435710  [57664/60000]\n",
      "Training:  Avg loss: 0.348361 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.395804 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.292618  [   64/60000]\n",
      "loss: 0.243663  [ 6464/60000]\n",
      "loss: 0.420169  [12864/60000]\n",
      "loss: 0.342700  [19264/60000]\n",
      "loss: 0.331847  [25664/60000]\n",
      "loss: 0.320887  [32064/60000]\n",
      "loss: 0.479597  [38464/60000]\n",
      "loss: 0.253586  [44864/60000]\n",
      "loss: 0.258528  [51264/60000]\n",
      "loss: 0.169278  [57664/60000]\n",
      "Training:  Avg loss: 0.347730 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.398393 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.335175  [   64/60000]\n",
      "loss: 0.295237  [ 6464/60000]\n",
      "loss: 0.423357  [12864/60000]\n",
      "loss: 0.493244  [19264/60000]\n",
      "loss: 0.481017  [25664/60000]\n",
      "loss: 0.451842  [32064/60000]\n",
      "loss: 0.257136  [38464/60000]\n",
      "loss: 0.375865  [44864/60000]\n",
      "loss: 0.452096  [51264/60000]\n",
      "loss: 0.417971  [57664/60000]\n",
      "Training:  Avg loss: 0.347113 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.398115 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.245972  [   64/60000]\n",
      "loss: 0.528497  [ 6464/60000]\n",
      "loss: 0.311243  [12864/60000]\n",
      "loss: 0.293433  [19264/60000]\n",
      "loss: 0.348068  [25664/60000]\n",
      "loss: 0.232475  [32064/60000]\n",
      "loss: 0.386275  [38464/60000]\n",
      "loss: 0.252282  [44864/60000]\n",
      "loss: 0.315490  [51264/60000]\n",
      "loss: 0.259878  [57664/60000]\n",
      "Training:  Avg loss: 0.346661 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.395046 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.527099  [   64/60000]\n",
      "loss: 0.364235  [ 6464/60000]\n",
      "loss: 0.378780  [12864/60000]\n",
      "loss: 0.470601  [19264/60000]\n",
      "loss: 0.513295  [25664/60000]\n",
      "loss: 0.249771  [32064/60000]\n",
      "loss: 0.208872  [38464/60000]\n",
      "loss: 0.395270  [44864/60000]\n",
      "loss: 0.421623  [51264/60000]\n",
      "loss: 0.334202  [57664/60000]\n",
      "Training:  Avg loss: 0.346067 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.395052 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.321240  [   64/60000]\n",
      "loss: 0.334869  [ 6464/60000]\n",
      "loss: 0.731681  [12864/60000]\n",
      "loss: 0.268462  [19264/60000]\n",
      "loss: 0.474857  [25664/60000]\n",
      "loss: 0.294723  [32064/60000]\n",
      "loss: 0.246591  [38464/60000]\n",
      "loss: 0.241544  [44864/60000]\n",
      "loss: 0.443441  [51264/60000]\n",
      "loss: 0.278035  [57664/60000]\n",
      "Training:  Avg loss: 0.345580 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.392425 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.287686  [   64/60000]\n",
      "loss: 0.448459  [ 6464/60000]\n",
      "loss: 0.422714  [12864/60000]\n",
      "loss: 0.369481  [19264/60000]\n",
      "loss: 0.495316  [25664/60000]\n",
      "loss: 0.356816  [32064/60000]\n",
      "loss: 0.243679  [38464/60000]\n",
      "loss: 0.287970  [44864/60000]\n",
      "loss: 0.495263  [51264/60000]\n",
      "loss: 0.242797  [57664/60000]\n",
      "Training:  Avg loss: 0.345209 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.394055 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.298861  [   64/60000]\n",
      "loss: 0.281510  [ 6464/60000]\n",
      "loss: 0.187443  [12864/60000]\n",
      "loss: 0.273207  [19264/60000]\n",
      "loss: 0.249160  [25664/60000]\n",
      "loss: 0.247989  [32064/60000]\n",
      "loss: 0.405416  [38464/60000]\n",
      "loss: 0.237813  [44864/60000]\n",
      "loss: 0.281227  [51264/60000]\n",
      "loss: 0.387411  [57664/60000]\n",
      "Training:  Avg loss: 0.344489 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.396336 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.293216  [   64/60000]\n",
      "loss: 0.296050  [ 6464/60000]\n",
      "loss: 0.339977  [12864/60000]\n",
      "loss: 0.286268  [19264/60000]\n",
      "loss: 0.296767  [25664/60000]\n",
      "loss: 0.480071  [32064/60000]\n",
      "loss: 0.401748  [38464/60000]\n",
      "loss: 0.379372  [44864/60000]\n",
      "loss: 0.305303  [51264/60000]\n",
      "loss: 0.264152  [57664/60000]\n",
      "Training:  Avg loss: 0.344043 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.392123 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.259582  [   64/60000]\n",
      "loss: 0.454879  [ 6464/60000]\n",
      "loss: 0.223111  [12864/60000]\n",
      "loss: 0.303314  [19264/60000]\n",
      "loss: 0.363748  [25664/60000]\n",
      "loss: 0.316855  [32064/60000]\n",
      "loss: 0.335300  [38464/60000]\n",
      "loss: 0.326552  [44864/60000]\n",
      "loss: 0.612683  [51264/60000]\n",
      "loss: 0.252728  [57664/60000]\n",
      "Training:  Avg loss: 0.343326 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.392077 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.366583  [   64/60000]\n",
      "loss: 0.360154  [ 6464/60000]\n",
      "loss: 0.331119  [12864/60000]\n",
      "loss: 0.356488  [19264/60000]\n",
      "loss: 0.319739  [25664/60000]\n",
      "loss: 0.417307  [32064/60000]\n",
      "loss: 0.459754  [38464/60000]\n",
      "loss: 0.376299  [44864/60000]\n",
      "loss: 0.284724  [51264/60000]\n",
      "loss: 0.527478  [57664/60000]\n",
      "Training:  Avg loss: 0.342853 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.392616 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.368101  [   64/60000]\n",
      "loss: 0.457978  [ 6464/60000]\n",
      "loss: 0.400040  [12864/60000]\n",
      "loss: 0.425394  [19264/60000]\n",
      "loss: 0.326641  [25664/60000]\n",
      "loss: 0.392617  [32064/60000]\n",
      "loss: 0.297484  [38464/60000]\n",
      "loss: 0.375451  [44864/60000]\n",
      "loss: 0.245420  [51264/60000]\n",
      "loss: 0.251382  [57664/60000]\n",
      "Training:  Avg loss: 0.342530 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.393303 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.255200  [   64/60000]\n",
      "loss: 0.305669  [ 6464/60000]\n",
      "loss: 0.359386  [12864/60000]\n",
      "loss: 0.462034  [19264/60000]\n",
      "loss: 0.281400  [25664/60000]\n",
      "loss: 0.606540  [32064/60000]\n",
      "loss: 0.203956  [38464/60000]\n",
      "loss: 0.322278  [44864/60000]\n",
      "loss: 0.404471  [51264/60000]\n",
      "loss: 0.415477  [57664/60000]\n",
      "Training:  Avg loss: 0.341730 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.392884 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.211299  [   64/60000]\n",
      "loss: 0.321596  [ 6464/60000]\n",
      "loss: 0.466312  [12864/60000]\n",
      "loss: 0.408475  [19264/60000]\n",
      "loss: 0.268307  [25664/60000]\n",
      "loss: 0.359125  [32064/60000]\n",
      "loss: 0.299222  [38464/60000]\n",
      "loss: 0.248670  [44864/60000]\n",
      "loss: 0.305400  [51264/60000]\n",
      "loss: 0.336026  [57664/60000]\n",
      "Training:  Avg loss: 0.341219 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.393066 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.159632  [   64/60000]\n",
      "loss: 0.530267  [ 6464/60000]\n",
      "loss: 0.245178  [12864/60000]\n",
      "loss: 0.270644  [19264/60000]\n",
      "loss: 0.284474  [25664/60000]\n",
      "loss: 0.386236  [32064/60000]\n",
      "loss: 0.515345  [38464/60000]\n",
      "loss: 0.476290  [44864/60000]\n",
      "loss: 0.245284  [51264/60000]\n",
      "loss: 0.374983  [57664/60000]\n",
      "Training:  Avg loss: 0.340936 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.390299 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.210355  [   64/60000]\n",
      "loss: 0.169455  [ 6464/60000]\n",
      "loss: 0.413346  [12864/60000]\n",
      "loss: 0.331464  [19264/60000]\n",
      "loss: 0.441453  [25664/60000]\n",
      "loss: 0.249361  [32064/60000]\n",
      "loss: 0.251459  [38464/60000]\n",
      "loss: 0.447629  [44864/60000]\n",
      "loss: 0.272755  [51264/60000]\n",
      "loss: 0.411295  [57664/60000]\n",
      "Training:  Avg loss: 0.340125 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.388798 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.311299  [   64/60000]\n",
      "loss: 0.236651  [ 6464/60000]\n",
      "loss: 0.404033  [12864/60000]\n",
      "loss: 0.497058  [19264/60000]\n",
      "loss: 0.243051  [25664/60000]\n",
      "loss: 0.284543  [32064/60000]\n",
      "loss: 0.281877  [38464/60000]\n",
      "loss: 0.225283  [44864/60000]\n",
      "loss: 0.255928  [51264/60000]\n",
      "loss: 0.222496  [57664/60000]\n",
      "Training:  Avg loss: 0.339501 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.394735 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.418925  [   64/60000]\n",
      "loss: 0.258170  [ 6464/60000]\n",
      "loss: 0.276943  [12864/60000]\n",
      "loss: 0.430110  [19264/60000]\n",
      "loss: 0.231923  [25664/60000]\n",
      "loss: 0.365659  [32064/60000]\n",
      "loss: 0.366770  [38464/60000]\n",
      "loss: 0.592562  [44864/60000]\n",
      "loss: 0.443489  [51264/60000]\n",
      "loss: 0.286564  [57664/60000]\n",
      "Training:  Avg loss: 0.339370 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.394847 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.413000  [   64/60000]\n",
      "loss: 0.361870  [ 6464/60000]\n",
      "loss: 0.279164  [12864/60000]\n",
      "loss: 0.239923  [19264/60000]\n",
      "loss: 0.387743  [25664/60000]\n",
      "loss: 0.284587  [32064/60000]\n",
      "loss: 0.309807  [38464/60000]\n",
      "loss: 0.221559  [44864/60000]\n",
      "loss: 0.306244  [51264/60000]\n",
      "loss: 0.183102  [57664/60000]\n",
      "Training:  Avg loss: 0.338405 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.390777 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.229190  [   64/60000]\n",
      "loss: 0.435898  [ 6464/60000]\n",
      "loss: 0.492539  [12864/60000]\n",
      "loss: 0.330173  [19264/60000]\n",
      "loss: 0.223214  [25664/60000]\n",
      "loss: 0.379430  [32064/60000]\n",
      "loss: 0.409302  [38464/60000]\n",
      "loss: 0.387258  [44864/60000]\n",
      "loss: 0.279311  [51264/60000]\n",
      "loss: 0.322439  [57664/60000]\n",
      "Training:  Avg loss: 0.338200 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.390285 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.242693  [   64/60000]\n",
      "loss: 0.272914  [ 6464/60000]\n",
      "loss: 0.339102  [12864/60000]\n",
      "loss: 0.317048  [19264/60000]\n",
      "loss: 0.391002  [25664/60000]\n",
      "loss: 0.339137  [32064/60000]\n",
      "loss: 0.469532  [38464/60000]\n",
      "loss: 0.368635  [44864/60000]\n",
      "loss: 0.259794  [51264/60000]\n",
      "loss: 0.461842  [57664/60000]\n",
      "Training:  Avg loss: 0.337543 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.389132 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.522285  [   64/60000]\n",
      "loss: 0.484307  [ 6464/60000]\n",
      "loss: 0.288436  [12864/60000]\n",
      "loss: 0.344852  [19264/60000]\n",
      "loss: 0.213397  [25664/60000]\n",
      "loss: 0.225524  [32064/60000]\n",
      "loss: 0.290670  [38464/60000]\n",
      "loss: 0.190897  [44864/60000]\n",
      "loss: 0.212834  [51264/60000]\n",
      "loss: 0.343007  [57664/60000]\n",
      "Training:  Avg loss: 0.337125 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.395066 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.199284  [   64/60000]\n",
      "loss: 0.363872  [ 6464/60000]\n",
      "loss: 0.355853  [12864/60000]\n",
      "loss: 0.236384  [19264/60000]\n",
      "loss: 0.377592  [25664/60000]\n",
      "loss: 0.529028  [32064/60000]\n",
      "loss: 0.278262  [38464/60000]\n",
      "loss: 0.206882  [44864/60000]\n",
      "loss: 0.274748  [51264/60000]\n",
      "loss: 0.440212  [57664/60000]\n",
      "Training:  Avg loss: 0.336621 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.389650 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.321467  [   64/60000]\n",
      "loss: 0.419616  [ 6464/60000]\n",
      "loss: 0.308159  [12864/60000]\n",
      "loss: 0.300151  [19264/60000]\n",
      "loss: 0.257875  [25664/60000]\n",
      "loss: 0.386261  [32064/60000]\n",
      "loss: 0.356529  [38464/60000]\n",
      "loss: 0.466659  [44864/60000]\n",
      "loss: 0.386437  [51264/60000]\n",
      "loss: 0.443858  [57664/60000]\n",
      "Training:  Avg loss: 0.336021 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.393951 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.419619  [   64/60000]\n",
      "loss: 0.319111  [ 6464/60000]\n",
      "loss: 0.207664  [12864/60000]\n",
      "loss: 0.250049  [19264/60000]\n",
      "loss: 0.268811  [25664/60000]\n",
      "loss: 0.344249  [32064/60000]\n",
      "loss: 0.387090  [38464/60000]\n",
      "loss: 0.270184  [44864/60000]\n",
      "loss: 0.448724  [51264/60000]\n",
      "loss: 0.297794  [57664/60000]\n",
      "Training:  Avg loss: 0.335737 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.387086 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.345497  [   64/60000]\n",
      "loss: 0.435078  [ 6464/60000]\n",
      "loss: 0.352022  [12864/60000]\n",
      "loss: 0.276783  [19264/60000]\n",
      "loss: 0.302744  [25664/60000]\n",
      "loss: 0.448951  [32064/60000]\n",
      "loss: 0.243786  [38464/60000]\n",
      "loss: 0.244047  [44864/60000]\n",
      "loss: 0.278818  [51264/60000]\n",
      "loss: 0.384288  [57664/60000]\n",
      "Training:  Avg loss: 0.334999 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.387320 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.323369  [   64/60000]\n",
      "loss: 0.295102  [ 6464/60000]\n",
      "loss: 0.215690  [12864/60000]\n",
      "loss: 0.404763  [19264/60000]\n",
      "loss: 0.304926  [25664/60000]\n",
      "loss: 0.269452  [32064/60000]\n",
      "loss: 0.274386  [38464/60000]\n",
      "loss: 0.266530  [44864/60000]\n",
      "loss: 0.405314  [51264/60000]\n",
      "loss: 0.324517  [57664/60000]\n",
      "Training:  Avg loss: 0.334781 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.385129 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 200\n",
    "save_loss = np.zeros((epochs,3))\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    save_loss[[t],[0]] = t+1\n",
    "    save_loss[[t],[1]] = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    save_loss[[t],[2]] = test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the development of the training and the test error through the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(save_loss[:,0], save_loss[:,1], label='Training')\n",
    "plt.plot(save_loss[:,0], save_loss[:,2], label='Test')\n",
    "# Adding legend, x and y labels, and titles for the lines\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training v Test Error')\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that initially the training and test errors drop together, but after around 25 epochs they srart seperating. Still, even afetr 200 epchs the test error is still dropping, although hardly any more.\n",
    "\n",
    "Before we will have a look at whether this model can now differentiate a trouser from a t-shirt and ankle boot, we shall save the model and its parameter. Depending on the power of your computer, it is likely that the above process took quite a while and if you can avoid it you do not want to redo this work (after all, how many cups of tea can you drink!).\n",
    "\n",
    "The model and its parameters can be saved as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_{}_{}'.format(int(datetime.timestamp(datetime.now())), epochs)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has now saved the model (specification and optimised parameters) in a file called `model_XXXX_200` where the `XXXX` will be replaced with a number representing the day and time at which you saved the model.\n",
    "\n",
    "The next time you open upir code and work at it, you do not have to re-estimate your model, but you can re-initiate it (which means that your model class definition has to have been run) and then you just reuploade the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model_1697438321_200'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how, before training the model, it was unable to detect that a picture depicted an ankle boot. So let's try this again. We feed the image back into the model (`ds_train[15][0]`) and check out the probabilities for the 10 clsses. Ankle boots are the last class so we are hoping for nine numbers close to 0 followed by a number close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3960e-09, 1.7297e-12, 1.2998e-11, 2.1133e-11, 5.8467e-12, 4.9630e-04,\n",
      "         1.2739e-09, 5.9815e-04, 1.0659e-06, 9.9890e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(ds_train[15][0])\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a success. The model tells us that there is a 99.89% probability that the item is indeed an ankle boot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
